{"title":"How Bad is Top-K Recommendation under Competing Content Creators?","markdown":{"yaml":{"title":"How Bad is Top-K Recommendation under Competing Content Creators?","subtitle":"Introduction for Newbies and Summary for the Rushed","categories":["machine learning"],"date":"2023-07-04","description":"Backround notes on recommenders from a game theory / bandit learning perspective to help people understand this week's paper.","aliases":["../topLK.html"]},"headingText":"Recommender Systems","containsRefs":false,"markdown":"\n\nPre-reading for the data science reading group as preparation for the paper [How Bad is Top-K Recommendation under Competing Content Creators?](https://arxiv.org/abs/2302.01971) (ICML 2023).\n\n\nA recommender can help a user reach desired content more quickly and discover new content they wouldn‚Äôt otherwise find at all. \n\nImagine the user follows many people and topics on mastodon. A recommender can rank followed content by analysing the user's past behaviour and interactions with the content. It can take into account factors such as how often and for how long the user interacts with certain content, what types of content the user tends to engage with more, and how similar the followed content is to other content the user has expressed interest in.\n\nTo recommend entirely novel content that they haven‚Äôt subscribed to, the recommender can use techniques such as collaborative filtering or content-based filtering. Collaborative filtering involves analysing information from other users with similar interests to make recommendations. Content-based filtering involves analysing the characteristics of the content itself to make recommendations based on similarities to other content the user has interacted with. Additionally, some recommenders may also use machine learning techniques to continuously learn and adjust recommendations based on user feedback.\n\nA top-K recommender returns the top K items; the paper assumes a ranking that best orders user satisfaction if recommended content is consumed. Previous work considered ranking according to expected user views (alone), ignoring whether the user liked the content (explicitly by liking it or implicitly by, eg., watching from start to video). \n\n\n## Resources\n\nGoogle [tutorial](https://developers.google.com/machine-learning/recommendation) on recommenders. \n\nArnie.\n\nFor Bruce: [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/abs/1205.2618)\n\n\n# Game Theory\n\nNash equilibrium is a concept in game theory where each player's strategy is optimal given the strategies of the other players. It is a state where no player can improve their payoff by unilaterally changing their strategy. \n\n\n![Prisoner's dilemna](prisoners-dilemna.png)\n\n\nConsider the Prisoner's Dilemma game, where two suspects are being interrogated separately for a crime they have committed together. Each suspect has the option to either confess or remain silent. If both remain silent, they both receive a light sentence. If one confesses and the other remains silent, the one who confesses receives a reduced sentence while the other receives a harsher sentence. If both confess, they both receive a moderate sentence.\n\nIn this game, the Nash equilibrium is for both suspects to confess, as neither has an incentive to remain silent if they believe the other will confess. This results in both suspects receiving a moderate sentence, even though they would have received a lighter sentence if they had both remained silent.\n\nIn some situations, players may benefit from coordinating their strategies, which leads to the concept of correlated equilibriums. A correlated equilibrium is a set of probability distributions over the players' strategies, where each player's strategy is optimal given the observed probability distribution. \n\nIn the prisoner's dilemma, the correlated equilibrium is a strategy profile where both players receive a signal that tells them which action to take. The signal is correlated with the other player's action, so that each player can infer the other player's action from their own signal. This allows the players to coordinate their actions and achieve a higher payoff (the lighter sentence) than they would in the Nash equilibrium.\n\nCoarse correlated equilibrium is a refinement of correlated equilibrium where players are allowed to use a limited communication protocol to coordinate their strategies. These concepts are important in understanding the strategic behaviour of individuals in various contexts, such as economics, politics, and social interactions. Imperfect communication can be modelled as noisy.\n\nA classic example of a coarse correlated equilibrium is the game of \"matching pennies.\" In this game, two players simultaneously choose to show either heads or tails on a flipped coin. If the two choices match, one player wins, and if they do not match, the other player wins. \n\nIn a coarse correlated equilibrium, the players agree to a randomization device, such as flipping a coin before the game begins, to determine their strategy. For example, Player 1 agrees to show heads if the coin lands on heads and tails if the coin lands on tails. Player 2 agrees to show heads if the coin lands on tails and tails if the coin lands on heads. \n\nThis strategy ensures that each player has a 50% chance of winning, regardless of the other player's choice. This is a coarse correlated equilibrium because the players are not coordinating their strategies directly but rather through a pre-agreed randomization device.\n\n\n## Shapley Values\n\nThe Shapley value treats a cooperative game as the sum of all possible combinations of player strategies. It calculates the individual contribution of each player by determining how much value they would generate if they were to play their optimal strategy, assuming that all other players are also playing their optimal strategy.\n\nIt quantifies how much value a player generates independently of the other players' strategies. It takes into account the marginal contribution of each player and provides a fair allocation of the total value among the players.\n\n\n## Resources\n\n2.1 Game theory overview of [Applications of game theory in deep learning: a survey](https://link.springer.com/article/10.1007/s11042-022-12153-2)\n\nSee also: [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)\n\n\n# Price of Anarchy\n\nThe recommender system controls what content the end user can choose from by ranking content either by the number of views (exposure) or by the total user satisfaction (engagement). A creator will strategize to rank highly and therefore try to maximise their number of views or the satisfaction of their viewers. Depending on the choice of ranking, the back and forth can lead to very poor user experience. The price of anarchy quantifies the overall diminished social welfare compared to a centrally controlled mediator that dictates the strategy for each creator to maximise overall user engagement.\n\n\n![Definition 2 from the paper](defn2.png)\n\n\nùí¢_ _is the game_, W_ is social welfare (or total user utility/engagement) for group strategy _s_. The numerator is the maximum social welfare over all possible joint strategies; the denominator is the worst case minimum social welfare when creators choose their own content (and follow a coarse correlated equilibrium). _PoA(ùí¢)_ ‚â• 1. [Because creators are constantly adding content and the users‚Äô tastes are changing too, in the real world a Nash equilibrium is very unlikely, eg. creator‚Äôs can view each other‚Äôs strategies and adjust their own. It‚Äôs imperfect at best ‚Äì they can observe each other but still not know what the other will do next ‚Äì squint and it may look like a coarse correlated equilibrium.]{.aside}\n\n\n# Reinforcement Learning\n\nWhile game theory is the math theory of strategic interactions between rational decision-makers, reinforcement learning is a type of machine learning where an _agent_ (player) learns from its _environment_ (game) through trial and error; the agent receives feedback in the form of _rewards_ or punishments for its _actions_ (possibly, chosen in accordance with a _policy_), and uses this feedback to adjust its behaviour to maximise its reward. \n\n\n![Reinforcement learning](rl.png)\n\n Our Agent receives **state S<sub>0</sub>‚Äã** from the **Environment** ‚Äî we receive the first frame of our game (Environment). Based on that **state S<sub>0</sub>,** the Agent takes **action A<sub>0</sub>‚Äã** ‚Äî our Agent will move to the right. The environment goes to a **new** **state S<sub>1</sub>‚Äã** ‚Äî new frame. The environment gives some **reward R<sub>1</sub>‚Äã** to the Agent ‚Äî we‚Äôre not dead _(Positive Reward +1)_.\n\n\n## Regret\n\nRegret is the difference between the expected reward of the optimal action (or the action of a predetermined, fixed policy) and the actual reward received by the agent for the actions taken. It‚Äôs often used as a performance metric of a reinforcement algorithm. A no-regret learning algorithm ensures that the agent's average regret approaches zero as it gains more experience in the environment. This is achieved by balancing exploration and exploitation of the environment, and by updating the agent's decision-making strategy based on the observed rewards. No-regret learning is particularly useful in settings where the environment is stochastic or uncertain, and where the optimal decision may change over time.\n\n\n## Resources\n\n[OpenAI's Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)\n\n[https://huggingface.co/learn/deep-rl-course](https://huggingface.co/learn/deep-rl-course)\n\n\n# Paper Summary\n\nThe paper explores the impact of competing content creators on user welfare in recommendation platforms. The authors model the creators‚Äô competition and user decisions, and examine how relevance-driven recommendation influences the dynamics in the long run. They find that the myopic approach to recommendation performs reasonably well in the long run, as long as users‚Äô decisions involve randomness and the platform provides reasonably many alternatives (K) to its users (if K=1 or user decisions are deterministic then the price of anarchy is ‚â• 2!). They also show that the fraction of user welfare loss due to creator competition is at most 1 + O( 1 /log K ) if K>1, user choices have mild stochastic noise and if creators are incentivized to compete for user engagement (eg, how long they watch the video) rather than user exposure (eg, whether they start the video). [User exposure can also be defined per player as how often their content is recommended to the user, regardless of the user‚Äôs choice among recommendations. But according to equations (2)-(4) and the discussion surrounding them, the authors define user exposure as the probability of the user also selecting that player‚Äôs content.]{.aside}","srcMarkdownNoYaml":"\n\nPre-reading for the data science reading group as preparation for the paper [How Bad is Top-K Recommendation under Competing Content Creators?](https://arxiv.org/abs/2302.01971) (ICML 2023).\n\n# Recommender Systems\n\nA recommender can help a user reach desired content more quickly and discover new content they wouldn‚Äôt otherwise find at all. \n\nImagine the user follows many people and topics on mastodon. A recommender can rank followed content by analysing the user's past behaviour and interactions with the content. It can take into account factors such as how often and for how long the user interacts with certain content, what types of content the user tends to engage with more, and how similar the followed content is to other content the user has expressed interest in.\n\nTo recommend entirely novel content that they haven‚Äôt subscribed to, the recommender can use techniques such as collaborative filtering or content-based filtering. Collaborative filtering involves analysing information from other users with similar interests to make recommendations. Content-based filtering involves analysing the characteristics of the content itself to make recommendations based on similarities to other content the user has interacted with. Additionally, some recommenders may also use machine learning techniques to continuously learn and adjust recommendations based on user feedback.\n\nA top-K recommender returns the top K items; the paper assumes a ranking that best orders user satisfaction if recommended content is consumed. Previous work considered ranking according to expected user views (alone), ignoring whether the user liked the content (explicitly by liking it or implicitly by, eg., watching from start to video). \n\n\n## Resources\n\nGoogle [tutorial](https://developers.google.com/machine-learning/recommendation) on recommenders. \n\nArnie.\n\nFor Bruce: [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/abs/1205.2618)\n\n\n# Game Theory\n\nNash equilibrium is a concept in game theory where each player's strategy is optimal given the strategies of the other players. It is a state where no player can improve their payoff by unilaterally changing their strategy. \n\n\n![Prisoner's dilemna](prisoners-dilemna.png)\n\n\nConsider the Prisoner's Dilemma game, where two suspects are being interrogated separately for a crime they have committed together. Each suspect has the option to either confess or remain silent. If both remain silent, they both receive a light sentence. If one confesses and the other remains silent, the one who confesses receives a reduced sentence while the other receives a harsher sentence. If both confess, they both receive a moderate sentence.\n\nIn this game, the Nash equilibrium is for both suspects to confess, as neither has an incentive to remain silent if they believe the other will confess. This results in both suspects receiving a moderate sentence, even though they would have received a lighter sentence if they had both remained silent.\n\nIn some situations, players may benefit from coordinating their strategies, which leads to the concept of correlated equilibriums. A correlated equilibrium is a set of probability distributions over the players' strategies, where each player's strategy is optimal given the observed probability distribution. \n\nIn the prisoner's dilemma, the correlated equilibrium is a strategy profile where both players receive a signal that tells them which action to take. The signal is correlated with the other player's action, so that each player can infer the other player's action from their own signal. This allows the players to coordinate their actions and achieve a higher payoff (the lighter sentence) than they would in the Nash equilibrium.\n\nCoarse correlated equilibrium is a refinement of correlated equilibrium where players are allowed to use a limited communication protocol to coordinate their strategies. These concepts are important in understanding the strategic behaviour of individuals in various contexts, such as economics, politics, and social interactions. Imperfect communication can be modelled as noisy.\n\nA classic example of a coarse correlated equilibrium is the game of \"matching pennies.\" In this game, two players simultaneously choose to show either heads or tails on a flipped coin. If the two choices match, one player wins, and if they do not match, the other player wins. \n\nIn a coarse correlated equilibrium, the players agree to a randomization device, such as flipping a coin before the game begins, to determine their strategy. For example, Player 1 agrees to show heads if the coin lands on heads and tails if the coin lands on tails. Player 2 agrees to show heads if the coin lands on tails and tails if the coin lands on heads. \n\nThis strategy ensures that each player has a 50% chance of winning, regardless of the other player's choice. This is a coarse correlated equilibrium because the players are not coordinating their strategies directly but rather through a pre-agreed randomization device.\n\n\n## Shapley Values\n\nThe Shapley value treats a cooperative game as the sum of all possible combinations of player strategies. It calculates the individual contribution of each player by determining how much value they would generate if they were to play their optimal strategy, assuming that all other players are also playing their optimal strategy.\n\nIt quantifies how much value a player generates independently of the other players' strategies. It takes into account the marginal contribution of each player and provides a fair allocation of the total value among the players.\n\n\n## Resources\n\n2.1 Game theory overview of [Applications of game theory in deep learning: a survey](https://link.springer.com/article/10.1007/s11042-022-12153-2)\n\nSee also: [SHAP](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)\n\n\n# Price of Anarchy\n\nThe recommender system controls what content the end user can choose from by ranking content either by the number of views (exposure) or by the total user satisfaction (engagement). A creator will strategize to rank highly and therefore try to maximise their number of views or the satisfaction of their viewers. Depending on the choice of ranking, the back and forth can lead to very poor user experience. The price of anarchy quantifies the overall diminished social welfare compared to a centrally controlled mediator that dictates the strategy for each creator to maximise overall user engagement.\n\n\n![Definition 2 from the paper](defn2.png)\n\n\nùí¢_ _is the game_, W_ is social welfare (or total user utility/engagement) for group strategy _s_. The numerator is the maximum social welfare over all possible joint strategies; the denominator is the worst case minimum social welfare when creators choose their own content (and follow a coarse correlated equilibrium). _PoA(ùí¢)_ ‚â• 1. [Because creators are constantly adding content and the users‚Äô tastes are changing too, in the real world a Nash equilibrium is very unlikely, eg. creator‚Äôs can view each other‚Äôs strategies and adjust their own. It‚Äôs imperfect at best ‚Äì they can observe each other but still not know what the other will do next ‚Äì squint and it may look like a coarse correlated equilibrium.]{.aside}\n\n\n# Reinforcement Learning\n\nWhile game theory is the math theory of strategic interactions between rational decision-makers, reinforcement learning is a type of machine learning where an _agent_ (player) learns from its _environment_ (game) through trial and error; the agent receives feedback in the form of _rewards_ or punishments for its _actions_ (possibly, chosen in accordance with a _policy_), and uses this feedback to adjust its behaviour to maximise its reward. \n\n\n![Reinforcement learning](rl.png)\n\n Our Agent receives **state S<sub>0</sub>‚Äã** from the **Environment** ‚Äî we receive the first frame of our game (Environment). Based on that **state S<sub>0</sub>,** the Agent takes **action A<sub>0</sub>‚Äã** ‚Äî our Agent will move to the right. The environment goes to a **new** **state S<sub>1</sub>‚Äã** ‚Äî new frame. The environment gives some **reward R<sub>1</sub>‚Äã** to the Agent ‚Äî we‚Äôre not dead _(Positive Reward +1)_.\n\n\n## Regret\n\nRegret is the difference between the expected reward of the optimal action (or the action of a predetermined, fixed policy) and the actual reward received by the agent for the actions taken. It‚Äôs often used as a performance metric of a reinforcement algorithm. A no-regret learning algorithm ensures that the agent's average regret approaches zero as it gains more experience in the environment. This is achieved by balancing exploration and exploitation of the environment, and by updating the agent's decision-making strategy based on the observed rewards. No-regret learning is particularly useful in settings where the environment is stochastic or uncertain, and where the optimal decision may change over time.\n\n\n## Resources\n\n[OpenAI's Spinning Up in Deep RL](https://spinningup.openai.com/en/latest/)\n\n[https://huggingface.co/learn/deep-rl-course](https://huggingface.co/learn/deep-rl-course)\n\n\n# Paper Summary\n\nThe paper explores the impact of competing content creators on user welfare in recommendation platforms. The authors model the creators‚Äô competition and user decisions, and examine how relevance-driven recommendation influences the dynamics in the long run. They find that the myopic approach to recommendation performs reasonably well in the long run, as long as users‚Äô decisions involve randomness and the platform provides reasonably many alternatives (K) to its users (if K=1 or user decisions are deterministic then the price of anarchy is ‚â• 2!). They also show that the fraction of user welfare loss due to creator competition is at most 1 + O( 1 /log K ) if K>1, user choices have mild stochastic noise and if creators are incentivized to compete for user engagement (eg, how long they watch the video) rather than user exposure (eg, whether they start the video). [User exposure can also be defined per player as how often their content is recommended to the user, regardless of the user‚Äôs choice among recommendations. But according to equations (2)-(4) and the discussion surrounding them, the authors define user exposure as the probability of the user also selecting that player‚Äôs content.]{.aside}"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":"cosmo","dark":["cosmo","../../theme-dark.scss"]},"max-width":"500px","title-block-banner":true,"author":"Lara Thompson","title":"How Bad is Top-K Recommendation under Competing Content Creators?","subtitle":"Introduction for Newbies and Summary for the Rushed","categories":["machine learning"],"date":"2023-07-04","description":"Backround notes on recommenders from a game theory / bandit learning perspective to help people understand this week's paper.","aliases":["../topLK.html"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}