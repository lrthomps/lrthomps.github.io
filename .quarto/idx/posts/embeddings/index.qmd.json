{"title":"Embeddings","markdown":{"yaml":{"title":"Embeddings","subtitle":"They aren’t just for text","categories":["machine learning"],"date":"2023-07-12","description":"Some introductory notes on embeddings as pre-reading for our data science paper reading group.","aliases":["../embeddings.html"]},"headingText":"Learning Embeddings","containsRefs":false,"markdown":"\n\n> “The representation perspective of deep learning is a powerful view that seems to answer why deep neural networks are so effective. Beyond that, I think there’s something extremely beautiful about it: why are neural networks effective? Because better ways of representing data can pop out of optimizing layered models.”\n> \n> from [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations)\n\nIn machine learning, an embedding is a technique for converting data objects (such as words or images), potentially sparse, into low-dimensional vectors where each vector represents its corresponding object. This conversion process allows us to use these vectors to perform various tasks such as classification or regression. Embeddings are used widely in many areas of machine learning, including natural language processing, computer vision, and graph analysis.\n\nOutside of textual data, for example, there are image embeddings, audio embeddings, and graph embeddings. For example, in image recognition tasks, CNNs learn image embeddings that capture visual patterns at different scales that can then be used for image classification, retrieval, or segmentation tasks.  In audio, recall in the VALL-E paper that they used a neural audio codec model trained to compress and decompress digital audio files. The intermediate encoding is an audio embedding.\n\nIn graph analytics, node and edge features can be combined to create graph embeddings that encode information about the structure and connectivity of the graph. These embeddings can be useful for solving problems like link prediction, community detection, or clustering. See [node2Vec](https://arxiv.org/abs/1607.00653) and [GraphSAGE](https://arxiv.org/abs/1706.02216). \n\n![GraphSage](graphsage.png)\n\n<p class=\"caption\">The “unrolled” equivalent neural network of <a href=\"https://github.com/dsgiitr/graph_nets/blob/master/GraphSAGE/GraphSAGE_Code%2BBlog.ipynb\">GraphSAGE</a></p>\n\nIn [recommender systems](https://eugeneyan.com/writing/system-design-for-discovery/), item embeddings reduce the dimensionality of the item catalog and allow for fast vector-search retrieval. New items can be cast into the item embedding space using similarity metrics based on item features before users have interacted with them at all. User embeddings alleviate the cold start problem in the same way.\n\n\n![Movie Embeddings in a Recommender](movie_embeddings.svg)\n\n<p class=\"caption\">A sample DNN architecture for learning movie embeddings from collaborative filtering data. From Google crash course on <a href=\"https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings\">embeddings</a></p>\n\nEmbeddings may finally allow neural network approaches to beat gradient boosted trees in tabular datasets:\n\n![Tabular embeddings that finally compete with tree algorithms](tabular-embeddings.png)\n\n<p class=\"caption\">First, continuous features are expanded into quantile bins to create higher dimensional sparse features; then, learned embeddings of these features allow the neural network to outperform CatBoost in a synthetic GBDT-friendly task. <a href=\"https://arxiv.org/abs/2203.05556\">https://arxiv.org/abs/2203.05556</a></p>\n\n\n\nMuch like the original [word2vec](https://www.tensorflow.org/tutorials/text/word2vec) or more modern language models, a surrogate task can be used to train embeddings: for sequences, predicting masked items (as in a [MLM](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling) like BERT) or predicting next items (as in a [causal language model](https://huggingface.co/docs/transformers/main/tasks/language_modeling) like the GPT family); for tabular data, the surrogate task can be the prediction of one column based on the others; for image data, predicting the category of image is a common task. In a recommender task, predicting the rating a user will assign an item is a good task if such labels are available; predicting an implicit signal such as if they’ll choose an item from the selection or how long they’ll watch a video once they start it. Often, items selected can form a sequence and many of methods from language modelling can be used, eg. [BERT4Rec](https://arxiv.org/abs/1904.06690).\n\nThe surrogate task doesn’t have to be the task you want the embeddings for; it should however depend on factors/features that are important for your downstream task. Eg. for the [chairs dataset](https://www.di.ens.fr/willow/research/seeing3Dchairs/), if your surrogate model classifies the chair orientation, the resulting embeddings would do poorly to predict the chair style. \n\nFurthermore, if you finetune your general purpose embeddings to a specific task, don’t expect them to still be useful for other tasks, see [here](https://itnext.io/changes-of-embeddings-during-fine-tuning-c22aa1615921) as an example.\n\n\n## Vectors Properties of Embeddings\n\nThe famous example popularized in the [word2vec paper](https://arxiv.org/abs/1301.3781) (but first appearing in [Linguistic Regularities in Continuous Space Word Representations](https://aclanthology.org/N13-1090/))\n\n<p style=\"text-align: center;\">King - Man + Woman = Queen</p>\n\n(though apparently, that expression requires [tweaking](https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85)). Why would the embeddings lie in a vector space? Frankly, the better question is: why wouldn’t they?\n\nSince the underlying models are overwhelmingly linear and frequently shallow (word2vec has a single hidden layer; GloVe embeddings approximate the full word co-occurrence matrix by a low rank decomposition), we should expect embeddings to lie in a vector space where similar items will be close. \n\n\n> “Representing features as different directions may allow _non-local generalization_ in models with linear transformations (such as the weights of neural nets), increasing their statistical efficiency relative to models which can only locally generalize.”\n> \n> from [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)[^1], they refer to a [paper by Bengio](https://arxiv.org/abs/1206.5538) and a [blog post](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations)\n\nThis linearity will only be broken when there are interactions among features. In natural language, words can combine to form an altogether different meaning: eg. “[wentelteefje](https://www.thedutchtable.com/2014/02/wentelteefjes.html)”; “to fast”; or idioms such as “to break the ice”, “to let the cat out of the bag” or “to table an issue”. But these examples are a minority, and overwhelmingly most writing (on the internet) is simple and “additive”. \n\n\n## References\n\n[Representation Learning Without Labels](https://icml.cc/virtual/2020/tutorial/5751), a well presented tutorial series at ICML 2020. \n\n\n<!-- Footnotes themselves at the bottom. -->\n## Notes\n\n[^1]:\n     [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) is an insightful writeup and I highly recommend reading!\n","srcMarkdownNoYaml":"\n\n> “The representation perspective of deep learning is a powerful view that seems to answer why deep neural networks are so effective. Beyond that, I think there’s something extremely beautiful about it: why are neural networks effective? Because better ways of representing data can pop out of optimizing layered models.”\n> \n> from [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations)\n\nIn machine learning, an embedding is a technique for converting data objects (such as words or images), potentially sparse, into low-dimensional vectors where each vector represents its corresponding object. This conversion process allows us to use these vectors to perform various tasks such as classification or regression. Embeddings are used widely in many areas of machine learning, including natural language processing, computer vision, and graph analysis.\n\nOutside of textual data, for example, there are image embeddings, audio embeddings, and graph embeddings. For example, in image recognition tasks, CNNs learn image embeddings that capture visual patterns at different scales that can then be used for image classification, retrieval, or segmentation tasks.  In audio, recall in the VALL-E paper that they used a neural audio codec model trained to compress and decompress digital audio files. The intermediate encoding is an audio embedding.\n\nIn graph analytics, node and edge features can be combined to create graph embeddings that encode information about the structure and connectivity of the graph. These embeddings can be useful for solving problems like link prediction, community detection, or clustering. See [node2Vec](https://arxiv.org/abs/1607.00653) and [GraphSAGE](https://arxiv.org/abs/1706.02216). \n\n![GraphSage](graphsage.png)\n\n<p class=\"caption\">The “unrolled” equivalent neural network of <a href=\"https://github.com/dsgiitr/graph_nets/blob/master/GraphSAGE/GraphSAGE_Code%2BBlog.ipynb\">GraphSAGE</a></p>\n\nIn [recommender systems](https://eugeneyan.com/writing/system-design-for-discovery/), item embeddings reduce the dimensionality of the item catalog and allow for fast vector-search retrieval. New items can be cast into the item embedding space using similarity metrics based on item features before users have interacted with them at all. User embeddings alleviate the cold start problem in the same way.\n\n\n![Movie Embeddings in a Recommender](movie_embeddings.svg)\n\n<p class=\"caption\">A sample DNN architecture for learning movie embeddings from collaborative filtering data. From Google crash course on <a href=\"https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings\">embeddings</a></p>\n\nEmbeddings may finally allow neural network approaches to beat gradient boosted trees in tabular datasets:\n\n![Tabular embeddings that finally compete with tree algorithms](tabular-embeddings.png)\n\n<p class=\"caption\">First, continuous features are expanded into quantile bins to create higher dimensional sparse features; then, learned embeddings of these features allow the neural network to outperform CatBoost in a synthetic GBDT-friendly task. <a href=\"https://arxiv.org/abs/2203.05556\">https://arxiv.org/abs/2203.05556</a></p>\n\n\n## Learning Embeddings\n\nMuch like the original [word2vec](https://www.tensorflow.org/tutorials/text/word2vec) or more modern language models, a surrogate task can be used to train embeddings: for sequences, predicting masked items (as in a [MLM](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling) like BERT) or predicting next items (as in a [causal language model](https://huggingface.co/docs/transformers/main/tasks/language_modeling) like the GPT family); for tabular data, the surrogate task can be the prediction of one column based on the others; for image data, predicting the category of image is a common task. In a recommender task, predicting the rating a user will assign an item is a good task if such labels are available; predicting an implicit signal such as if they’ll choose an item from the selection or how long they’ll watch a video once they start it. Often, items selected can form a sequence and many of methods from language modelling can be used, eg. [BERT4Rec](https://arxiv.org/abs/1904.06690).\n\nThe surrogate task doesn’t have to be the task you want the embeddings for; it should however depend on factors/features that are important for your downstream task. Eg. for the [chairs dataset](https://www.di.ens.fr/willow/research/seeing3Dchairs/), if your surrogate model classifies the chair orientation, the resulting embeddings would do poorly to predict the chair style. \n\nFurthermore, if you finetune your general purpose embeddings to a specific task, don’t expect them to still be useful for other tasks, see [here](https://itnext.io/changes-of-embeddings-during-fine-tuning-c22aa1615921) as an example.\n\n\n## Vectors Properties of Embeddings\n\nThe famous example popularized in the [word2vec paper](https://arxiv.org/abs/1301.3781) (but first appearing in [Linguistic Regularities in Continuous Space Word Representations](https://aclanthology.org/N13-1090/))\n\n<p style=\"text-align: center;\">King - Man + Woman = Queen</p>\n\n(though apparently, that expression requires [tweaking](https://blog.esciencecenter.nl/king-man-woman-king-9a7fd2935a85)). Why would the embeddings lie in a vector space? Frankly, the better question is: why wouldn’t they?\n\nSince the underlying models are overwhelmingly linear and frequently shallow (word2vec has a single hidden layer; GloVe embeddings approximate the full word co-occurrence matrix by a low rank decomposition), we should expect embeddings to lie in a vector space where similar items will be close. \n\n\n> “Representing features as different directions may allow _non-local generalization_ in models with linear transformations (such as the weights of neural nets), increasing their statistical efficiency relative to models which can only locally generalize.”\n> \n> from [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)[^1], they refer to a [paper by Bengio](https://arxiv.org/abs/1206.5538) and a [blog post](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations)\n\nThis linearity will only be broken when there are interactions among features. In natural language, words can combine to form an altogether different meaning: eg. “[wentelteefje](https://www.thedutchtable.com/2014/02/wentelteefjes.html)”; “to fast”; or idioms such as “to break the ice”, “to let the cat out of the bag” or “to table an issue”. But these examples are a minority, and overwhelmingly most writing (on the internet) is simple and “additive”. \n\n\n## References\n\n[Representation Learning Without Labels](https://icml.cc/virtual/2020/tutorial/5751), a well presented tutorial series at ICML 2020. \n\n\n<!-- Footnotes themselves at the bottom. -->\n## Notes\n\n[^1]:\n     [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) is an insightful writeup and I highly recommend reading!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":"cosmo","dark":["cosmo","../../theme-dark.scss"]},"max-width":"500px","title-block-banner":true,"author":"Lara Thompson","title":"Embeddings","subtitle":"They aren’t just for text","categories":["machine learning"],"date":"2023-07-12","description":"Some introductory notes on embeddings as pre-reading for our data science paper reading group.","aliases":["../embeddings.html"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}