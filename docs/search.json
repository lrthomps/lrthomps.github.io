[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CV-ish",
    "section": "",
    "text": "CURRENT\n\n\n\n\n\n\n\n\nPrincipal Machine Learning Scientist / Engineer at Tableau / Salesforce. I started out on the Explain Data feature team then I was briefly on the Einstein Discovery team. Since the summer, I’ve been working on Tableau Pulse developing insights that are the heart of Tableau Pulse and architecting new GAI features.\nBouldering more and more (regularly getting up to ~V3-V5; also finding harder and harder V2). Ran the Enchantments in-a-day (many hours); ran a semi-fast 10km (48:20).\n\n\n\n\nPREVIOUS\n\n\n\n\n\n\n\n\nMy first gig outside of academia. I was hired to improve their appliance disaggregation (separating the real+reactive electrical signal of a house into the constituent appliances). My first prototype outperformed their deployed version but it was based on edge detection and edge pairing, and didn’t work in many homes (e.g. with heaters that cycle on and off at high frequency and varying amplitude). A U-Net model with attention could learn to separate individual appliances well but we still needed data per appliance type, sometimes even per make and model (after heating and cooling, dryers and fridges are the biggest power consumers and both are fairly distinctive, but not always; e.g. the sharp initial spike of an old fridge cycling on is nearly completely suppressed in an ultra-efficient modern fridge).\nHome electricity monitoring enables smarter solar + battery usage: I devised control algorithms for various tariff structures. E.g. Solar panels produce peak power midday while most homes have peak consumption in the evenings; storing the excess in a battery for later, when tariffs are higher, offsets the cost and better utilizes the solar generation. For all tariffs, I compared against optimal in-hindsight control to quantify regret; mostly, optimal control is possible in most homes because the battery is generally small enough it’s always worth fully charging and saving for the evening. By the end of my three years I was leading an ML team of 4 and had run a decent half marathon (1:49:29).\n\n\n\n\nPOSTDOC\n\n\n\n\n\n\n\n\nBack to UBC, joining Kurt Haas’s neurioscience lab to do analysis work. I started attending a data science paper reading meetup that I still attend / help organize; find us on discord.\nClimbed Mt Rainier (Disappointment Cleaver router) and Pico de Orizaba (with lots of fresh snow!) with MITOCers.\n\n\n\n\n&lt;intermission&gt;\n\n\n\n\n\n\n\nSpent time in the climbing mountains; house sitting on Galiano, working as a barista at the local bakery. Though I still cannot make proper foam, I now make an excellent pour over.\n\n\n\n\nPOSTDOC\n\n\n\n\n\n\n\n\nPostdoc with Patrick Lee looking for quantum spin glass signatures. This work involved some tricky numerics to theoretically support measured experimental signatures in a candidate quantum spin glass. MIT was such an incredible place with hallways full of science experiments, students studying and random dancing.\nI taught at the winter school run by MITOC: ice climbing (that I was new to!), winter hiking and camping (in a quinzhee rather than the snow caves I’d tested in BC mountains); then lead climbing in the summer.\n\n\n\n\nMSc/PhD\n\n\n\n\n\n\n\n\nMSc / PhD in Condensed Matter Theory with Philip Stamp. My thesis on the “Equation of Motion of a Quantum Vortex” was deemed by D. J. Thouless (as external reviewer of my thesis) to resolve the decades-long dispute between his group and researchers in Russia: not in his favour!\nJoined the VOC, taught at glacier school, fall long hike (a climbing school); took up backcountry skiing, mountaineering, and trad climbing. For a year, I played on the Junior Varsity volleyball team (as offside) before my fingers demanded I choose between volleyball and climbing. Climbed a bunch of mountains (Matier, Joffre, Rainier, Wedge), skied a few traverses (Garibaldi Neve, Spearhead in-a-day, Wapta icefield), led a few climbs (Snake, Over the Rainbow, Exasperator) and one boulder (Tumbleweed V2, though I tried many others: bouldering is hard).\n\n\n\n\nBMath / BSc\n\n\n\n\n\n\n\n\nBSc / BMath double in Physics & Applied Math w/t Pure Math Minor w/t co-op. Often taking 6 courses + labs, I squeezed an extra physics degree into my math undergrad with just one extra 2 course term (during which I continued research from the summer)."
  },
  {
    "objectID": "posts/how-bad-is-top-K/index.html",
    "href": "posts/how-bad-is-top-K/index.html",
    "title": "How Bad is Top-K Recommendation under Competing Content Creators?",
    "section": "",
    "text": "Pre-reading for the data science reading group as preparation for the paper How Bad is Top-K Recommendation under Competing Content Creators? (ICML 2023)."
  },
  {
    "objectID": "posts/how-bad-is-top-K/index.html#resources",
    "href": "posts/how-bad-is-top-K/index.html#resources",
    "title": "How Bad is Top-K Recommendation under Competing Content Creators?",
    "section": "Resources",
    "text": "Resources\nGoogle tutorial on recommenders.\nArnie.\nFor Bruce: BPR: Bayesian Personalized Ranking from Implicit Feedback"
  },
  {
    "objectID": "posts/how-bad-is-top-K/index.html#shapley-values",
    "href": "posts/how-bad-is-top-K/index.html#shapley-values",
    "title": "How Bad is Top-K Recommendation under Competing Content Creators?",
    "section": "Shapley Values",
    "text": "Shapley Values\nThe Shapley value treats a cooperative game as the sum of all possible combinations of player strategies. It calculates the individual contribution of each player by determining how much value they would generate if they were to play their optimal strategy, assuming that all other players are also playing their optimal strategy.\nIt quantifies how much value a player generates independently of the other players’ strategies. It takes into account the marginal contribution of each player and provides a fair allocation of the total value among the players."
  },
  {
    "objectID": "posts/how-bad-is-top-K/index.html#resources-1",
    "href": "posts/how-bad-is-top-K/index.html#resources-1",
    "title": "How Bad is Top-K Recommendation under Competing Content Creators?",
    "section": "Resources",
    "text": "Resources\n2.1 Game theory overview of Applications of game theory in deep learning: a survey\nSee also: SHAP"
  },
  {
    "objectID": "posts/how-bad-is-top-K/index.html#regret",
    "href": "posts/how-bad-is-top-K/index.html#regret",
    "title": "How Bad is Top-K Recommendation under Competing Content Creators?",
    "section": "Regret",
    "text": "Regret\nRegret is the difference between the expected reward of the optimal action (or the action of a predetermined, fixed policy) and the actual reward received by the agent for the actions taken. It’s often used as a performance metric of a reinforcement algorithm. A no-regret learning algorithm ensures that the agent’s average regret approaches zero as it gains more experience in the environment. This is achieved by balancing exploration and exploitation of the environment, and by updating the agent’s decision-making strategy based on the observed rewards. No-regret learning is particularly useful in settings where the environment is stochastic or uncertain, and where the optimal decision may change over time."
  },
  {
    "objectID": "posts/how-bad-is-top-K/index.html#resources-2",
    "href": "posts/how-bad-is-top-K/index.html#resources-2",
    "title": "How Bad is Top-K Recommendation under Competing Content Creators?",
    "section": "Resources",
    "text": "Resources\nOpenAI’s Spinning Up in Deep RL\nhttps://huggingface.co/learn/deep-rl-course"
  },
  {
    "objectID": "posts/data-where/index.html",
    "href": "posts/data-where/index.html",
    "title": "Data is everywhere!",
    "section": "",
    "text": "Last verified October 19, 2024 (though the datasets seem dated!)\nLast night I was on a data science career panel (of awesome ladies!) as part the Vancouver Datajam 2020 and I promised (as I’ve been meaning to do for a while…) to post a list of data resources. The hardest part of finding data isn’t finding such a list but finding such a list that is up-to-date but I’ll try. Should I fail here first is a list of well maintained curated data source lists."
  },
  {
    "objectID": "posts/data-where/index.html#an-aggregate-of-other-aggregators",
    "href": "posts/data-where/index.html#an-aggregate-of-other-aggregators",
    "title": "Data is everywhere!",
    "section": "An Aggregate of Other Aggregators",
    "text": "An Aggregate of Other Aggregators\n\nHuggingface datasets\nPyTorch Vision and PyTorch NLP\nTensorflow models/datasets resource is offered by Google. Many of the datasets below are accessible via tensorflow_datasets\nUCI ML Repository “currently maintain 557 data sets as a service to the machine learning community”\nKaggle including such gems as the arXiv and avocado prices\nGoogle Public Data is curating datasets; they also have a Dataset Search\nOpenSpending: “search over 3,446 data packages from 83 countries with over 159,706,407 fiscal records”\nHarvard Dataverse is a repository for research data (and code!).\nFiveThirtyEight posts all the data to back the articles\nA compilation of Twitter/X datasets\nTableau Public hosts datasets\nStats Canada; DataBC; Vancouver Open Data; US Data.gov; NYC OpenData; Seattle Open Data; Our World in Data; etc…\nAppen hosts some Open Source Datasets\nKDnuggets has datasets galore and also aggregates yet more aggregators. Alas, some links are out of date.\n\n\nOk, but how to divvy up the data types?\nUltimately I have a taxonomy problem: divide the data by datatype, domain or best-suited algorithm type? Finally, I’ll do a mixture of all three. This is how my mind divides them; this is how I ultimately search among them; this is hopefully how such a list will be most useful."
  },
  {
    "objectID": "posts/data-where/index.html#curated-datasets",
    "href": "posts/data-where/index.html#curated-datasets",
    "title": "Data is everywhere!",
    "section": "Curated Datasets",
    "text": "Curated Datasets\nA breed all their own: they’re uniform, tidy, split into training/validation/test sets, (over-)used to pit algorithms against each other (some curated and shared for that purpose but aren’t adopted as readily). Older benchmarks are good for starting out or for hard variants of the problem statement (eg. one-shot!). See Papers With Code hosted SOTA by benchmark.\n\nDisentanglement/Representation Learning\n\nMPI3D datasets simulated and real-world environments\ndisentanglement_lib includes dSprites, Color/Noisy/Scream-dSprites, SmallNORB, Cars3D, and Shapes3D\nalso, try generating points on a surface in 3d to represent in 2d, such as the swiss roll (more rolls are harder to learn)\n\n\n\nImages\n\nMNIST, CIFAR-10/100, and Fashion-MNIST all have ~60k images split among 10 classes\nciFAIR-10/100 duplicate free versions of CIFAR-10/100\nImageNet is large with bigger images a decent subset annotated with bounding boxes\nDanBooru2021 a large-scale anime image database with 4.9m+ images annotated with 162m+ tags\nLarge-scale Fashion (DeepFashion) Database to scale up Fashion-MNIST\nPlant Disease is the most widely used in agriculture studies\nUnsplash lite and Full\nZappos50K 4 categories of shoes\nUCSD Birds 200 categories of birds\nCelebA 200K images each with 40 attributes\nVisual Domain Decathlon 10 simultaneous visual challenges\n\n\n\nSegmentation & Captioning\n\nStreet View House Numbers (SVHN)\nCOCO is a large-scale object detection, segmentation, and captioning dataset\nOpen Images image labels, bounding boxes, segmentation, relations, and narratives\nVisualQA (VQA) open-ended questions about images requiring an understanding of vision, language and commonsense knowledge to answer\n\n\n\nNLP\n\nLarge Movie Review Dataset and Sentiment140 for sentiment analysis \nTwenty Newsgroups for text classification\ncurrated Wikipedia Corpus or dumps from Wikipedia itself\nBlog Authorship Corpus many blogs of many bloggers\nMachine Translation ~15GB within various “tasks”\nYelp Open Dataset mixes NLP with images, interaction timelines, coordinates\nOne Billion Words a standard corpus of reasonable size (0.8 billion words)\nFake News Corpus\nPG-19 extracted from Gutenberg\nSnowden archive\nDarknet Market Archives 2013-2015 scrapes covering vendor pages, feedback, images, etc.\n3m Russian Troll tweets from FiveThirtyEight\n\nSentiWordNet (“assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity”) may be interesting to compare against in sentiment analysis from supervised datasets.\n\nNLU\n\nSQuAD 1-2 datasets\nGLUE and SuperGlue\nMeasuring Massive Multitask Language Understanding bigger, harder to test GPT-3\nand so many many more since the explosion of LLMs…\n\n\n\nRecommenders\n\nMS Learning to Rank dataset\nMovieLens 25m ratings for ~60k movies of ~160k users\nSpotify Recsys Challenge 2018 assembled by MSc students independent of Spotify who no longer host it\nGoodbooks-10k scraped from GoodReads\nBook-Crossing\nNetflix Prize, a classic\nGroupLens links to various datasets (book crossing is on Kaggle! look back two links)\n\n\n\nVarious\n\nPenn ML Benchmarks for supervised learning algorithms\nAutoML/AutoDL competitions datasets dating back to 2016; Springer has open access to the book with a chapter reviewing the challenge\nOKCupid dataset N=68,371, 2,620 variables from the dating site OKCupid\nCommon Crawl has petabytes of data, regularly collected since 2008\nGDELT Project “watching our world unfold”, or (less creepy) “the GDELT Project monitors the world’s broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, themes, sources, emotions, counts, quotes, images and events driving our global society every second of every day, creating a free open platform for computing on the entire world.”\nCS bibliography has many datasets in many domains\n\n\n\nOutlier/Anomaly/Event Detection\n\nOn the Evaluation of Unsupervised Outlier Detection data\nOutlier Detection Datasets (ODDS)\nUnsupervised Anomaly Detection Benchmark data\nAnomaly Detection Meta-Analysis Benchmarks data\nNumenta Anomaly Benchmark (NAB)\nTuring Change Point Dataset\n*MAVEN: A massive general domain event detection dataset, and accompanying paper.\n\n\n\nOne/Few Shot\n\nminiImageNet was introduced in Matching Networks for One Shot Learning; Meta-Transfer Learning for Few-Shot Learning added tieredImageNet and Fewshot-CIFAR10 both available to downloaded directly; also see mini on Kaggle\nMeta-Dataset assembles various datasets into one benchmark\nChollet’s ARC-AGI dataset and a recent (ongoing as I write this in 2024) competition\n\n\n\nGraphs\n\nStanford Large Network Dataset Collection for social graphs, roads, communication networks and more\nOpen Graph Benchmark (OGB) for “a collection of realistic, large-scale, and diverse benchmark datasets”\nOpenStreetMap\nSketchGraphs “A Large-Scale Dataset for Modeling Relational Geometry in Computer-Aided Design”\nData for STREETS\n2013 NYC Taxi Trip Data\n\n\n\nSymbolic Regression\nBoth from the universe of Max Tegmark: * AI Feynman all eqns from the Feynman lectures, includes bonus eqns * AI Physicist considers different forces per region of space;\n\n\nAudio\n\nFree Spoken Digit Dataset = spoken MNIST\nSpeech Command Dataset with 65k 1s utterances of 30 short spoken commands like “Yes”, “No”, “Stop”, “Go”\nFree Music Archive ~900GB/343 days of Creative-Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres\nMillion Song Dataset audio features and metadata of ~1m popular music tracks\nLibriSpeech ~1k hrs of audiobooks from LibriVox\nVoxCeleb ~1m utterances by ~7k celebrities, &gt;2k hrs\nSpotify OpenMic; their podcast dataset TREC is no longer available\n\nand Video: AViD collected videos with a creative-commons license shared as a static dataset"
  },
  {
    "objectID": "posts/data-where/index.html#data-in-the-wild",
    "href": "posts/data-where/index.html#data-in-the-wild",
    "title": "Data is everywhere!",
    "section": "Data in the Wild",
    "text": "Data in the Wild\n\nTime Series\n\nS&P 500\nSpotify Sequential Skip Prediction Challenge but this has pages of User Agreements to scroll+click through\nCompEngine a self-organizing db of time-series data\n\n\n\nClimate data\n\nGlobal climate data\nNOAA\nwww.data.gov/climate\nAI for Earth may help with resources\nCatalyst Cooperative\nWashington Post Data behind the series “2ºC: Beyond the Limit.”, also here, here, and here.\n\n\nRecommended by Amanda Giang during the discussion: * Pangeo and the dataset WeatherBench that they host * Zenodo * Google Earth Engine\n\n\n\nSports stats\n\nNHL\nFormula-1\nbaseball\nBBC athletes dataset"
  },
  {
    "objectID": "posts/going-jekyll-free-2020.html",
    "href": "posts/going-jekyll-free-2020.html",
    "title": "Site overhaul to get rid of Jekyll",
    "section": "",
    "text": "I had great intentions last summer to add more regular posts: more paper summaries, worked through ideas, etc. But [excuses]. Fast forward to now, I have notes to put up but have since switched entirely from a mac to Ubuntu, discovered a far better app to edit markdown with math, Typora (not freeware but so good I’ll pay once it’s out of beta [looking back in 2024: I did!]), and I couldn’t get the new post to render properly (some strange auto-scaling was happening).\n\n\nAfter I wasted over nearly two days debugging Jekyll (cause stuff happened outside of my template files), I gave up. In a couple of hours, I was rid of Jekyll, rid of frameworks and down to raw html+css. Gotta say, html5 once you give in to it, is damn nice compared to circa 2004 wed development (my work is still live [still live in 2024], don’t look).\n\n\nA few things to share to anyone who’s thinking of doing the same:\n\n\n\n\nbefore you do, keep in mind it worked for me because\n\n\n\nI know html+css but not Jekyll\n\n\nI don’t need a real blog (with feed, commenting, auto-generated file structure etc); though all I’m missing is the feed.xml file (except… see the next section)\n\n\n\n\n\nI copied code snippets from other sites, kept some css from the Jekyll minima theme, but my goal was to strip it back to the simplest it could be\n\n\n\n\ngist for syntax highlighting of code, and let Github host and render\n\n\n\n\nmathjax for latex math: I try to keep formulas from going too wide but if they must I choose to let them run over the page edge on small devices. It’s better than squinting at scaled down math\n\n\n\n\ncolours: again, simple is best. Think of all the wonderful people that use a dark mode: does half your content disappear? It seems the majority of pages still have a white(ish) background, so I went with that too: it means the simplest dark modes that invert colours works for my site (e.g. Firefox iOS app).\n\n\n\n\nMy favourite find to convert a picture of myself that didn’t feel so […]: cartoonize project/paper + demo. It’s another style transfer GAN, predominantly trained on scenery images are collected from Shinkai Makoto, Miyazaki Hayao and Hosoda Mamoru films. The cityscapes are the best, especially those with clouds. I used a picture from the approach on Mount Rainier: love what it did to the rocks, grass, trees and background hills!\n\n\n…and bringing Jekyll back\n\nI considered getting the feed working manually, adding posts manually hereon, updating posts.html manually. But finally, I accepted the challenge of just getting Jekyll working after all. It was too much like installing a GPU for ML only — fighting against the grain — except in this case, why bother?\nMain takeaways:\n\nno theme is required: I kept fighting the theme, elements kept not looking like they should. Overall, I failed the theme. But! no theme required! I had all the layout files anyway. Lesson learned. No theme.\nthe theme I got rid of wanted assets in _assets but normal Jekyll wants them in assets\nmath: use for pipes; for complex conjugate. Otherwise, pipes are interpreted as part of a table and stars italicize\nposts didn’t have to get buried in folder hell: specify the file structure in _config.yml:\npermalink: /posts/:title.html\nfeed.xml doesn’t make it easy to sign up in the first place (though it does provide the actual feed): add to the header of the root layout\n&lt;link href='{{ \"feed.xml\" | relative_url }}' rel='alternate' type='application/atom+xml'&gt;\nFor some reason, to sign up with Feedly, I have to enter https://lrthomps.github.io/posts.html to be found (not feed.xml).\n\nMy Jekyll configuration is bare bones. By all means, copy as much as you like!"
  },
  {
    "objectID": "posts/gpu-for-ml-only.html",
    "href": "posts/gpu-for-ml-only.html",
    "title": "GPU for ML only on Ubuntu",
    "section": "",
    "text": "I fought with NVIDIA drivers taking over my graphics and xterm on Ubuntu 20.04. It was either have my GPU do everything and lose up to half its memory in the process… or nothing. I had it working once… then updates uninstalled the driver and broke everything again.\nThat was a few weeks ago. I’m confident now, a few updates later, that I’m using the correct driver and that settings are all good, so I’ll document what it took. Some may be unnecessary steps but since I got it working once and haven’t experimented further, I can’t say.\n\nin BIOS settings, I had to enable iGPU multi-monitor and enable CPUgraphics\ninstall CUDA tools\nsudo apt install nvidia-cuda-toolkit\ninstall cudnn from NVIDIA; requires an account\ninstall a headless driver and accompanying utils\nsudo apt install nvidia-headless-440 nvidia-utils-440\nadd to PATH and LD_LIBRARY_PATH in .bashrc\nexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64\ncreate a new config file in /etc/X11/xorg.conf with settings\nSection \"ServerLayout\"\n    Identifier     \"Layout0\"\n    Screen      0  \"Screen0\"\nEndSection\n\nSection \"Screen\"\n    Identifier     \"Screen0\"\n    Device         \"intel\"\nEndSection\n\nSection \"Device\"\n    Identifier      \"intel\"\n    Driver          \"intel\"\n    VendorName      \"Intel Corporation\"\n    BusId           \"PCI:0:2:0\"\n    Option \"AccelMethod\" \"sna\"\n    Option \"TearFree\" \"true\"\n    Option \"DRI\" \"3\"\nEndSection\nmodify grub settings in /etc/default/grub so that\nGRUB_CMDLINE_LINUX=\"nogpumanager\"\nthen run sudo update-grub for the settings to take effect\nif, in the process, the file /usr/share/X11/xorg.conf.d/11-nvidia-prime.conf was created: delete it\nfinally, last but not least, following random/good internet advice, add (create?) the following to /etc/modprobe.d/blacklist-nvidia.conf\nblacklist nvidia-drm\nalias nvidia-drm off\n\nNow all should be working. To be sure, running nvidia-smi should show something like\nFri Sep 11 19:45:45 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 2070    Off  | 00000000:02:00.0 Off |                  N/A |\n| 35%   47C    P0    22W / 175W |      0MiB /  7982MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nAnd, in e.g. a jupyter notebook, running tf.config.list_physical_devices('GPU') should output the\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nEureka! All the memory for ML!"
  },
  {
    "objectID": "posts/embeddings/index.html",
    "href": "posts/embeddings/index.html",
    "title": "Embeddings",
    "section": "",
    "text": "“The representation perspective of deep learning is a powerful view that seems to answer why deep neural networks are so effective. Beyond that, I think there’s something extremely beautiful about it: why are neural networks effective? Because better ways of representing data can pop out of optimizing layered models.”\nfrom Deep Learning, NLP, and Representations\nIn machine learning, an embedding is a technique for converting data objects (such as words or images), potentially sparse, into low-dimensional vectors where each vector represents its corresponding object. This conversion process allows us to use these vectors to perform various tasks such as classification or regression. Embeddings are used widely in many areas of machine learning, including natural language processing, computer vision, and graph analysis.\nOutside of textual data, for example, there are image embeddings, audio embeddings, and graph embeddings. For example, in image recognition tasks, CNNs learn image embeddings that capture visual patterns at different scales that can then be used for image classification, retrieval, or segmentation tasks. In audio, recall in the VALL-E paper that they used a neural audio codec model trained to compress and decompress digital audio files. The intermediate encoding is an audio embedding.\nIn graph analytics, node and edge features can be combined to create graph embeddings that encode information about the structure and connectivity of the graph. These embeddings can be useful for solving problems like link prediction, community detection, or clustering. See node2Vec and GraphSAGE.\nThe “unrolled” equivalent neural network of GraphSAGE\nIn recommender systems, item embeddings reduce the dimensionality of the item catalog and allow for fast vector-search retrieval. New items can be cast into the item embedding space using similarity metrics based on item features before users have interacted with them at all. User embeddings alleviate the cold start problem in the same way.\nA sample DNN architecture for learning movie embeddings from collaborative filtering data. From Google crash course on embeddings\nEmbeddings may finally allow neural network approaches to beat gradient boosted trees in tabular datasets:\nFirst, continuous features are expanded into quantile bins to create higher dimensional sparse features; then, learned embeddings of these features allow the neural network to outperform CatBoost in a synthetic GBDT-friendly task. https://arxiv.org/abs/2203.05556"
  },
  {
    "objectID": "posts/embeddings/index.html#learning-embeddings",
    "href": "posts/embeddings/index.html#learning-embeddings",
    "title": "Embeddings",
    "section": "Learning Embeddings",
    "text": "Learning Embeddings\nMuch like the original word2vec or more modern language models, a surrogate task can be used to train embeddings: for sequences, predicting masked items (as in a MLM like BERT) or predicting next items (as in a causal language model like the GPT family); for tabular data, the surrogate task can be the prediction of one column based on the others; for image data, predicting the category of image is a common task. In a recommender task, predicting the rating a user will assign an item is a good task if such labels are available; predicting an implicit signal such as if they’ll choose an item from the selection or how long they’ll watch a video once they start it. Often, items selected can form a sequence and many of methods from language modelling can be used, eg. BERT4Rec.\nThe surrogate task doesn’t have to be the task you want the embeddings for; it should however depend on factors/features that are important for your downstream task. Eg. for the chairs dataset, if your surrogate model classifies the chair orientation, the resulting embeddings would do poorly to predict the chair style.\nFurthermore, if you finetune your general purpose embeddings to a specific task, don’t expect them to still be useful for other tasks, see here as an example."
  },
  {
    "objectID": "posts/embeddings/index.html#vectors-properties-of-embeddings",
    "href": "posts/embeddings/index.html#vectors-properties-of-embeddings",
    "title": "Embeddings",
    "section": "Vectors Properties of Embeddings",
    "text": "Vectors Properties of Embeddings\nThe famous example popularized in the word2vec paper (but first appearing in Linguistic Regularities in Continuous Space Word Representations)\n\nKing - Man + Woman = Queen\n\n(though apparently, that expression requires tweaking). Why would the embeddings lie in a vector space? Frankly, the better question is: why wouldn’t they?\nSince the underlying models are overwhelmingly linear and frequently shallow (word2vec has a single hidden layer; GloVe embeddings approximate the full word co-occurrence matrix by a low rank decomposition), we should expect embeddings to lie in a vector space where similar items will be close.\n\n“Representing features as different directions may allow non-local generalization in models with linear transformations (such as the weights of neural nets), increasing their statistical efficiency relative to models which can only locally generalize.”\nfrom Toy Models of Superposition1, they refer to a paper by Bengio and a blog post\n\nThis linearity will only be broken when there are interactions among features. In natural language, words can combine to form an altogether different meaning: eg. “wentelteefje”; “to fast”; or idioms such as “to break the ice”, “to let the cat out of the bag” or “to table an issue”. But these examples are a minority, and overwhelmingly most writing (on the internet) is simple and “additive”."
  },
  {
    "objectID": "posts/embeddings/index.html#references",
    "href": "posts/embeddings/index.html#references",
    "title": "Embeddings",
    "section": "References",
    "text": "References\nRepresentation Learning Without Labels, a well presented tutorial series at ICML 2020."
  },
  {
    "objectID": "posts/embeddings/index.html#notes",
    "href": "posts/embeddings/index.html#notes",
    "title": "Embeddings",
    "section": "Notes",
    "text": "Notes"
  },
  {
    "objectID": "posts/embeddings/index.html#footnotes",
    "href": "posts/embeddings/index.html#footnotes",
    "title": "Embeddings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nToy Models of Superposition is an insightful writeup and I highly recommend reading!↩︎"
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html",
    "href": "posts/assess-yourself-for-better-bouldering/index.html",
    "title": "Assess Yourself for Better Bouldering",
    "section": "",
    "text": "This summer, I got stronger with some “baseBase training is meant to increase training volume slowly, improve general fitness, and lay a good foundation for sport-specific training. A better base leads into more effective training (theoretically).” training, pull-ups, push-ups and core circuits, and brokeI’ve reached 3-hex multiple times through my climbing years never with any training, just through climbing; with training, I went from 4 to 10 pull-ups / 10 to 15 push-ups / 7 to 30s in a hanging L-sit; from struggling on half the 3-hexes to reliably getting a few 4-hexes. I should mention that I’ve been this strong and fit before but never climbing at the grade! So climbing practice over the many years has meant I mostly just need to be stronger (this time). my 3-hex (V2-V4) plateau. Instead of continuing with climbingUsually split to first build strength and then to build power. specific training, I spent a few months adjusting to that new strength; aka, reaping the benefits. After a decade of being warned against hangboardingOn the common fear of hangboarding, in Beastmaking, Ned Feehally writes, “[fingerboard training] is the safest way of training finger strength as it is very controlled, you can apply the load slowly and let go at any time”. and (worse) campus boardingOn the ubiquity of campus boarding, Ned writes, “modern climbing walls have developed so far and become so good that the campus board seems massively dated;” instead, he suggests campusing routes and training on a system board.—“you’ll injure yourself!”—why not just continue with the same training? It works!\nThe first two hints against this plan come from the 9c “ultimate” climbing test and the Climbing Finger Strength Analyzer 2.0:\nLooks like my core and pulling training have already outpaced my finger training.\nConsider a parallel storyDrawn from The Science and Practice of Strength Training of a shot putter training to throw a 7kg shot. He trains to bench press more and more, 50kg then 150kg—it’s improving his throws!—200kg, 300kg—and yet he can throw no further. He got stronger, yes, but what he needed was more power.\nBut what is the ideal strength profile of a boulderer? How much finger strength? How much pulling strength? How much power? More is surely better but what is a good balance progressing from V-lo through V-mid to V-hi? What about push-ups, legs and endurance? Surely it depends on my specific goals and likely on my height, arm span, and hand size also, but general estimates would still help guide my training.\nIn this post, I’ll analyse and model a dataset collected from a survey on Reddit, including many strengthHalf-crimp and open hand 10s max hangs on 18mm edge; 5-rep pull-up max weight; max hanging L-sit time; # pull-ups / push-ups. metrics, trainingTraining specifics included hangboard protocols (eg. repeaters, max hangs, min edges, one-armed protocols, etc.), grip types trained (eg. half crimp, open hand, etc.); endurance training protocols (4x4, threshold intervals, feet on campusing, etc.); non-specific training (upper body pulling and pushing, core, legs and antagonists); other activities (running, yoga, skiing, martial arts, etc); and the number of sessions and hours per week doing various kinds of training. habits and basic climber attributesHeight, weight, arm span, years climbing, indoors vs outdoors, and various bouldering and climbing grades.. We’ll see how these climbers balance and progress their strengths through the grades; how they train at each stage; and see that height and BMI both have a sweet spotMost aspects in a complex system have a ‘sweet spot’: any time there are competing factors at play, the optimum is somewhere in the middle when neither penalty is too extreme. Weight scales very loosely as a cube of height; muscles is quadratic in height. If this were the only factor at play, then, in bodyweight sports, shorter would be strictly better (think gymnastics): this would give a max strength scaling with weight with power 0.666, analysing world record athletes finds a power of 0.63 – close! In climbing, reach is still important and increases with height linearly. Balancing the two opposing factors, we should find an optimal height and corresponding weight. BMI is weight over height squared so it too should increase for similarly strong athletes (in a bodyweight sport) with increasing height and it too will have an optimum by the same arguments., not too low, not too high.\nWhat about mobilityMobility is the stability and strength through your full range of motion.? Although certainly quantifiable, no mobility assessments are in the dataset I’ll be using, so my analysis will not consider it, but more on this later."
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#meet-the-climbers",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#meet-the-climbers",
    "title": "Assess Yourself for Better Bouldering",
    "section": "Meet the Climbers",
    "text": "Meet the Climbers\nSpecial guests include: Megan, better known as @fearlesstofu in social media, and her strong climbing friend, Nelson @aloeveraelephant. Our data points will be highlighted against the anonymous climbers in the survey to show our relative strengths and weaknesses; you can compare findings against examples of our climbing, Megan especially on her YouTube channel or on Instagram.\n\n\nMeganNelsonLara\n\n\n\n\n\n\n\n\n\n\n\nMegan on her first outdoor V6: Fat Tire in South Lake Tahoe\n\n\n\n\n\nMetrics\n\n156cm tall\n\n\nneutral ape index\n\n\n20.8 BMI\n\n\nclimbing ~6 years\n\nGrades\n\nV8 indoors\n\n\nV6 indoors in the last 3 months\n\n\nV7 outside\n\n\nV4 is nearly always doable indoors\n\nTraining\n\n2 hrs / week on a moonboard\n\nStrength Metrics\n\n+12kg half crimp\n\n\n+26.26kg open hand\n\n\n+4kg 5rep pull-ups\n\n\n10 pull-ups\n\n\n14 push-ups\n\n\n38s hanging L-sit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNelson on a V6 in Columbia, CA called Triple Cracks\n\n\n\n\n\nMetrics\n\n175cm tall\n\n\n+4cm ape index\n\n\n22.5 BMI\n\n\nclimbing ~5 years\n\n\nclimbs outdoors “maybe 2x / year”\n\nGrades\n\nV11\n\n\nV8, in the last 3 months\n\n\nV7 is nearly always doable\n\nTraining\n\n10 hrs climbing / week\n\n\nno other training\n\nStrength Metrics\n\n+36kg half crimp\n\n\n+48kg open hand\n\n\n+28kg 5rep pull-ups\n\n\n18 pull-ups\n\n\n24 push-ups\n\n\n27s hanging L-sit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMe on the very wet Brothers Creek Erratic along the Baden Powell trail.\n\n\n\n\n\nMetrics\n\n168cm tall\n\n\n-4cm ape index\n\n\n22.4 BMI\n\n\nclimbing 18 years\n\n\nbouldering ~7 years\n\nGrades\n\n4-hex (V4-V6) at the Hive\n\n\nV5 on the kilter board\n\n\nV3 outside / outdoor V1 are nearly always doable\n\nTraining\n\n6-8 hrs climbing / week\n\n\n1-2 hr training / week\n\n\nran ~1200km in 2021\n\nStrength Metrics\n\n+30lbs half crimp\n\n\n+4lbs open hand\n\n\n+25lbs 5rep pull-ups\n\n\n10 pull-ups\n\n\n19 push-ups\n\n\n30s hanging L-sit\n\n\n\n\n\n\n\n\n\nThe survey included another 513 men and 77 women. V6 is the most common grade for these climbers who are typically in their first 4 years of climbing. Climbing grades rise with the number of years and plateau after around 5 years. Higher V grade climbers are more likely to climb outsideNearly everyone in this dataset climbs inside but about half of the ‘strong’ climbers also climb outside. I started with trad climbing and came indoors during thesis time. For those that started inside, I strongly suggest trying bouldering on real rock. It’ll challenge your route reading infinitely more, the footwork is almost always harder and the variety of grips you’ll practice is endless.. The women’s V grades cluster at V3, V5 and V9, familiar as common plateaus; generally, in the much larger and wider dataset at Lattice Training, these fall at V6 and V8.\nHigher V grade male climbers tend to be shorter while higher V grade female climbers tend to be taller; they asymptotically meet in the middle:\n\nThe ideal height is ~170cmAlex Megos, 173cm; Nalle Hukkataival, 173cm; Daniel Woods, 170cm; Stefano Ghisolfi, 170cm; Colin Duffy, 168cm. Akiyo Noguchi, 168cm; Janja Garnbret, 164cm; Shauna Coxsey, 163cm; Oriane Bertone, 164cm. But: Jimmy Webb, 183cm; Adam Ondra, 185cm; Jan Hojer, 188cm; Dean Potter, 196cm; and: Sean Bailey, 163cm; Brooke Raboutou, 158cm; Natalia Grossman, 157cm; Lynn Hill, 157cm; Ashima Shiraishi, 155cm; Laura Rogora, 152cm. and +5-10 ape index.—Adam Ondra\n\n…though he discusses the merits of short vs tallShort climbers must learn creative solutions early for lack of reach and puts them on a path for better technique. Being tall isn’t that big an advance – it’s the long arms that it comes with that are. Overall, some climbs suit short people better; others suit tall people better. Climb to your strengths!, long vs short armsLong arms are good for their long reach but worse in every other aspect: harder lock-offs, harder underclings, require more leverage and more torque., and being a little light…for better endurance sport climbing vs a little more muscled…to be a stronger boulderer. here.\nThere are more climbers with +veApparently this generalizes to non-climbers: up to 5% longer is normal (the climbers here average 1% longer arm span than height). There are regional and gender differences, and shrinkage with age [Quanjer, 2014]. index than -ve. BMIBMI is a crude scaling of weight and height (about as accurate as when the physicist ‘approximates the cow by a sphere’). It doesn’t differentiate muscle from fat and muscle is 20% heavier per volume than fat. That said, for many problems, the scaling works, and the cow may as well be a sphere. trends downward with increasing V grade in men (from &gt;25 to 22) while in women the trend tips slightly upward (from &lt;18 to ~21). An optimal BMI for bouldering may be ~21-22Only two boulderers have climbed V17: Daniel Woods, 21.2; and Nalle Hukkataival, 22.7; I can’t get (adult) numbers for any V15 female climbers (Ashima Shiraishi has climbed 2; Kaddi Lehman and Oriane Bertone have each climbed one); strong V14 climbers include Shauna Coxsey, 21.8; and Alex Puccio, 23.1. Others that are primarily boulderers that commonly make ‘top 10’ lists: Guiliano Cameroni, 19.6; Jimmy Webb, 23.3; Dai Koyomada, 21.3; Adam Ondra, 20.7..\nThe women are on average lighter (58kg vs 72kg), shorter (165cm vs 179cm), have &gt;1cm shorter ape index, have been climbing 4 months less, climb 15min less per week, train 20min less per week, and are weaker in all measures except they can hold an L-sit 10s longer; they climb 1-2 V grades lower but their hardest grade is closer to the grade they can climb 90-100% of the time.\nThe highest V grade climbers climb roughly twice as much per week as beginners. Beginners don’t train at all, while the highest V grade climbers train 5-8 hours per week on average.\nIn the table, you can seeIf you have trouble understanding these plots and you happen to also climb at the Hive, track me down and I might be able to help. these statistics visually (click through to see all the figures).\n\n\nGradesIndoor vs OutdoorsYearsGrade vs YearsHeightApe IndexBMIClimbingTraining\n\n\n\n\n\nLooking at the number of climbers by max recent V grade, V6 / V3 are the most common grades for men / women, but we have climbers up to V14 / V9.\n\n\n\n\n\n\n\nIf we break down those grades by who climbs indoors vs outdoors we find that most strong climbers (also) climb outside whereas the majority of beginners only climb inside. A few men only climb outside.\n\n\n\n\n\n\n\nThe number of climbers that have climbed more than 4 years drops off sharply. Megan has 6 years and Nelson has 5. For me, ~3-18 years.\n\n\n\n\n\n\n\nThere’s a wide spread of grades among climbers with the same number of years of climbing experience but the average grade by year shows an upward trend. The women’s curve looks noisy because there aren’t enough respondents.\n\n\n\n\n\n\n\nA wide range of heights is common for the middling grades but the scatter funnels down to around 170cm. Adam Ondra and Ashima Shiraishi are strong outliers.\n\n\n\n\n\n\n\nApe index trends higher with increasing grades but the scatter is 5x wider so I won’t let my -4cm be an excuse (though my arm span is shorter than Ashima’s!).\n\n\n\n\n\n\n\nLooking at BMI vs V grade, the men trend downward while the women trend slowly upward; the noise narrows just as height did.\n\n\n\n\n\n\n\nLeft: Climbers by # of hours climbing per week is fairly noisy with 4/6 hours being the most common for women/men. Right: Climbers at higher grades climb more.\n\n\n\n\n\n\n\nLeft: Climbers by # of hours spent training (of all kinds) is noisier still with &lt; 6/8 most likely for women/men. Right: Climbers at higher grades train more."
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#not-all-surveyed-v-grades-are-the-same",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#not-all-surveyed-v-grades-are-the-same",
    "title": "Assess Yourself for Better Bouldering",
    "section": "Not All Surveyed V Grades are the Same",
    "text": "Not All Surveyed V Grades are the Same\nClimbers submitted three different grades: max ever, recent max (in the last 3 months ), and the grade that’s “doable” 90-100% of the time. Grading gets more complicated when we consider indoor vs outdoor (or moonboard vs kilter board for that matter).\nMax ever V grade is problematic: the climber may have sent it years ago and their current strength and habits aren’t what they once were. The spread is as wide as 4 V grades for a climber of 15 years and, generally, widerAlthough a few climbers claimed the opposite: that their recent max is higher than their max ever. Presumably, they read the survey questions wrong and I reversed their grades. at higher V grades.\nThe doable grade is highly subjective. Should we consider all styles? Or just the style we love and climb well. What about the reachy climbs if you’re short, or the overly compressed climbs if you’re tall? Just how peaky is your climbing pyramid? Do you always get beta from others before trying? The spread from doable grade to max ever is as wide as 5 V grades; on average, the spread is wider at the higher grades. Doable grade against max recent shows the same wideningA few climbers claim higher doable grade than max recent, surely impossible. I filtered out the climbers that answered ‘I don’t boulder’ for either question. If they’re bouldering at all, they could have climbed at least one problem at their doable grade. spread.\n\nMax vs Max RecentDoable vs MaxDoable vs Max Recent\n\n\n\n\n\nWhen we compare max ever V grade to hardest in the last 3 months, the spread is as high as 4 V grades. The spread widens with more years climbing (tiny inset).\n\n\n\n\n\n\n\nThe doable V grade vs max ever V grade is funnel-shaped: the wider gaps are more common at higher grades. Megan may be underestimating or her height may disadvantage her on too many climbs.\n\n\n\n\n\n\n\nDoable vs max recent V grade is again funnel-shaped. The few (blue) points above the dashed (1:1) line claimed a higher doable grade than max recent.\n\n\n\n\n\nAll plots and analysis hereafter will consider only max recent V grade."
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#how-strong-are-the-climbers",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#how-strong-are-the-climbers",
    "title": "Assess Yourself for Better Bouldering",
    "section": "How Strong are the Climbers?",
    "text": "How Strong are the Climbers?\nFinger strength is the most correlated feature we have to climbing grade. The numbers are similar for men and women and agreeThough Lattice Training uses 7s/20mm max hangs vs 10s/18mm max hangs gathered by this survey. fairly well with the climbers from Lattice Training: each V grade requires adding another 8% of body weight to max hangs; to climb 9c would require ~1.1That’s a hang with 110% of bodyweight added, or 210% total weight.. Silence, the first 9c, was put up by Adam Ondra with max hangs of 112%; Bibliographie, briefly the second 9c, was first climbed by Alex Megos whose max hangs are an astonishing 132%.\nWhile finger strength didn’t show a big gender difference, the normalized weight added to 5 pull-ups is 10-20% moreEither the women make do with less pulling strength or they prefer problems that don’t require it. for male climbers at any given grade. Pulling strength increases with grade up to around 0.880% of bodyweight, in agreement with the pulling to finger strength ratio recommended by the Climbing Finger Strength Analyzer 2.0 (0.75) and the Lattice Training finger strength estimate of 1.1 to climb 9c, where 0.8 ~ 0.75*1.1. at the highest grades. For any given grade, the male climbers seem to do twice as many pull-ups or push-ups as the women. Despite many over-achievers, more modest goals of 20-25 pull-ups and 30-40 push-ups seem sufficient for climbing. Less than 25% submitted their L-sit time and many suspiciouslyWere they just guessing or did they get bored and give up? answered an even 30s and 60s and 12 with times exceeding the world record.\nHere are the same metrics visually; like before, click through the tabs to see them all.\n\nFinger Strength5-rep Weighted Pullups# Pullups / PushupsL-sit (s)\n\n\n\n\n\nBoth Megan and Nelson seem to have more finger strength than their recent grade merits, but both fall perfectly on the trendline using their max ever V grades (8, 11).\n\n\n\n\n\n\n\nFor 5-rep pull-up strength, the women trend a steady 0.2 (or 20% of bodyweight) below the men. Ideally, this strength should be less than finger strength and we’ll see later that, in higher V grade climbers, it is.\n\n\n\n\n\n\n\nNumber of pull-ups (left) and push-ups (right) by grade. A further eight male climbers are cut off in the right plot, one claiming 286 push-ups!\n\n\n\n\n\n\n\nSelf-reported L-sit times went as high as 2h46m; 12 in total exceeded the world record of 1m29s (during COVID raised to 1m46s, not by a climber). Less than 25% reported. Be suspicious of conclusions based on it.\n\n\n\n\n\n\n\n\n\n\nReproduced with the permission of Lattice Training Ltd.\n\n\nWhat about differences in strength metrics: how does finger strength compare with pulling strength? And how do they compare as we get stronger? How do half crimp and open hand compare for finger strength?\nWhen finger and pulling strength are still low, pulling strength exceeds finger strength in both women and men. As they increase, women quickly develop far moreMegan has more than 6x finger strength than pulling strength! stronger fingers; the men develop equal finger strength only when both are ~50% body weight. In the strongest climbers, finger strength settles at roughly double pulling strength.\nThe difference between half crimpNot to be confused with full crimp that engages the thumb. and open handNot to be confused with 3 finger drag that drops the pinky. max hangs may average out to zero but less than a third of those35% of climbers gave half crimp max hangs, 28% gave open hand max hangs while 25% gave both: which tips the balance another 10% in favour of half crimps. To compare, 46% gave their 5-rep pull-up max weight; 29% gave both weighted pull-ups and a max hang in either grip. reporting both had them within 5%The arbitrary metric used for acceptable left/right strength asymmetry seems like a reasonable buffer to use here. of each other. In Beastmaking, Ned suggests that hand anatomy plays a role in which grip a climber favours. Full- and half-crimp are generally the stronger grip but open hand places less strain on the finger pulleys.\n\nFinger vs Pulling StrengthOpen vs Half Crimp\n\n\n\n\n\nThe ratio of finger strength to 5-rep pull-ups plotted against finger strength (1:1, equal strengths, along the dashed line). Climbers in this plot provided both weighted pull-ups and a max hang in either grip (leaving only 233/16 men/women).\n\n\n\n\n\n\n\nThe difference between open and half crimp strengths is very noisy but mostly averages out; there’s a slight trend to stronger open hand in the stronger climbers (only 136/9 men/women provided both max hangs)."
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#bouldering-predictions",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#bouldering-predictions",
    "title": "Assess Yourself for Better Bouldering",
    "section": "Bouldering Predictions",
    "text": "Bouldering Predictions\nWith this dataset of nearly 600Dropping a few with suspicious data including those claiming to have broken world records. climbers, I could train a modelSeveral in fact, of various kinds: I started with linear regression, went to 2-4 layer dense neural networks before settling on random forests then boosted trees (which, of course, are usually best for tabular data like this). I trained many of each kind, keeping various subsets of climber features, and settled on a model that gave consistent feature influences. to predict the hardest recent V grade of each climber given their strength metrics and training habits. The model has only mild regression to the meanLow V grade climbers are predicted high, while high V grade climbers are predicted low., surprising since there were many metrics that weren’t provided; typical errors were &lt;2 V grades.\n\n\nA simple way to assess feature importance is with correlation with grade (numbers are using only climbers that provide that feature). The model can better utilize a feature if more climbers provide it (percentage that did in brackets):\n\n\nopen hand strength, 0.64 (28%)\n\n\nhalf crimp strength, 0.59 (35%)\n\n\n# pull-ups, 0.49 (78%)\n\n\nyear, 0.43 (all)\n\n\nhrs climbing per week, 0.32 (all)\n\n\n# hangboard/campus board sessions per week, 0.34/0.3 (all, 56%/26% do)\n\n\nL-sit time, 0.31 (31%)\n\n\n5-rep pull-up strength, 0.29 (46%)\n\n\nhrs training per week, 0.26 (all, 46% train endurance, 79% train strength)\n\n\n# push-ups, 0.13 (61%)\n\n\nBMI, -0.13 (all)\n\n\nThe feature importanceCalculated by shuffling the values of that feature among all climbers and seeing how much worse the predictions are on average. The range each feature can take varies widely, and shuffling can give an entirely unreasonable shift in a given climber. Years vary from 0 to 15 and BMI from 15 to 30, and yet most of us can’t wait a year to improve and fret over BMI increments of a few pounds. Compare that to hrs on the campus board per week that varies from 0 to 5 where half an hour can make a difference. estimates how much the model relies on it for predictions; feature influence is the average gains for a standard small shift in that feature.\nBy far the most important feature is how many years a climber has been climbing and, next in importance, is the number of hours climbing per week. After all:\n\nClimbing is the best training for climbing\n\nThe top strength metrics by any measure are the number of pull-ups, half crimp strength, and the number of hours campus board training per week. Campus board training is synonymous with training for powerPower is a measure of the rate of force applied; think of contact strength and how much harder it is to catch a bad hold in a lunge than to grab it statically, or think of the fast pulling strength to initiate that lunge. so we can say:\n\nFinger and pulling strength and power are the best predictors of climbing grade\n\nConsistently important and somewhat influential, we find BMI. For climbers with BMI &lt; 20 (&gt;25), raising (lowering) BMI always increases the expected grade; in between, predictions go up or down depending on the individual. The largest prediction changes were for climbers that didn’t provide many strength metrics; for the three of us that did, BMI influence was tinyI saw the largest gains: by lowering my BMI by 0.5 (aka, losing 3lbs), my predicted grade increases by 0.17..\nBoth feature assessments entirely ignore the correlationAnd among all the other features too. Features that are strongly correlated: number of pull-ups and pull-up strength (0.50), number of pull-ups and number of push-ups (0.44), years climbing and half crimp strength (0.34), BMI and number of push-ups (0.24), etc. of BMI to the strength metrics: being lighterFurthermore, the denominator in the normalized metrics further amplifies any weight loss. makes all body-weight exercises easier; while having more muscle more than compensates for the added weight. There’s a limit in both lines of thinking: we should only lose excess fat and muscle and keep a safe weight for optimal recovery; we should only gain muscle beneficial to climbing and carefully balance muscle strength. To quote Tomoa from Beastmaking:\n\nAfter trying to reinforce a specific skill or train a body part, it is found that it made the total balance of the body corrupted. —Tomoa Narasaki\n\nTo estimate the maximum expected gains of each climber, we can add up all the positive influences. These gains diminish with grade; but, luckily (if true), Megan, Nelson and I have high estimated gains for our V grade.\n\nFeature InfluenceFeature ImportanceMax GainsPredictions\n\n\n\n\n\nThe feature influence considers the change in expected grade for a shift in a given feature. An extra half hour on the campus board has the greatest effect.\n\n\n\n\n\n\n\nThe permutation importance of each feature gives the (normalized) increase in mean squared error when that feature is randomly shuffled. Given that the error nearly doubles, ‘years’ is a very important feature.\n\n\n\n\n\n\n\nWith the step shifts used to assess feature influence, adding up the positive steps each climber can take defines their maximal gains. The gains per climber go down with grade.\n\n\n\n\n\n\n\nThis lightgbm model was trained holding out a random 30 climbers (in green) including Megan, Nelson and me: that the test climbers are predicted as well as the others is a sign the model trained well.\n\n\n\n\n\nThe influence of each feature depends on the profile of a given climber. Here are the gains by feature for Megan, Nelson and I as examples. Of course, we won’t suddenly climb harder by changingThe influence of changing training and getting stronger is not something this dataset can answer since it only has a single snapshot of each climber in their present state. training habits; rather, these are the nearest profiles to us that the model expects to have the biggest jump in grade and we might expect good results if we emulate them (and we trust the model and the dataset).\n\nMeganNelsonLara\n\n\n\n\n\nThe expected feature influence for Megan’s profile: more pull-ups; power training; and increase both grips with emphasis on her dominant open hand grip.\n\n\n\n\n\n\n\nThe expected feature influence for Nelson’s profile: biggest gains to strengthen his half crimp; start power training; and increase his L-sit time. The predicted gains from losing 1.5kg don’t seem worth it.\n\n\n\n\n\n\n\nThe expected feature influence for my profile: power training, more pull-ups, increase half crimp strength, climb more, increase 5-rep pull-up strength, lose 3lbs, and more push-ups."
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#your-data-next",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#your-data-next",
    "title": "Assess Yourself for Better Bouldering",
    "section": "Your Data Next",
    "text": "Your Data Next\nWe’ve studied these climbers and, although we haven’t discovered an ideal training progression, rough strength scales emerged that agree with intuition and the training literature. Are you more like Megan with &gt;6x stronger fingers than pulling strength? Or like the handful of climbers that can add nearly their full bodyweight to 5 pullups and yet only climb V4? Assess yourself to see your strength profile: how balanced is it and how does it compare to the grades you’re climbing? As you progress, how does your climbing improve with increased strength?\n\n\n\nOngoing data to collect\n\n\n\nindoor, system board and outdoor grades: new milestones and sends/attemptsAttempts are good to track, especially for problems eventually climbed, but not all attempts are equal. Sends are definitely more stable to track. per session (as sum(grades)Maximize this to train for endurance. and avg(grades)Maximize this to train for power and better technique. Best to increase sum(V) first with doable grades, then to hold that steady as try to increase avg(V).)\n\n\ngrade by the style of climb (slab, overhang, compression, dynamic)\n\n\ntraining sessions, protocol details, and how it felt\n\n\nday by day, how do you feel? tired, aching? strong!\n\n\nresting heart rate each morning\n\n\nsleep quality\n\n\nTest as many aspects as you like, but do not trainThey fail to serve if you start to train to them specifically (max hangs being an exception), aka you overfit if you train to the test. For instance, ‘core’ is more than the muscles of your torso; it’s the entire chain of muscles that lets you hold tension from hands to feet. Assessing with hanging L-sits doesn’t cover the entire chain; the front lever comes closer, but neither covers the full variety of positions you’ll hopefully get into bouldering. to them. Assess once per training cycle to influence how you train in the next one; twice a year to monitor your progress if you’re training by climbing.\n\nIf you are still getting stronger and better by just climbing, then just climb!\n\nFrom one assessment to the next, look for trends in your response to different training stimuli. When you start to plateau with a given exercise, try a more advanced progression, if any, or just switch things up (eg. max hangs to repeaters). Monitor your load capacity and increaseIf training gets you fitter and hence capable of more training (hurray!) or you’re sleeping and eating better, and practicing active recovery./decreaseBecause other stressors in life come up, or you have more opportunity to go climbing! Or you have a trail race coming up that you also have to train for. as necessary. It’s safer to aim low and recover well for each session than to aim high and risk injury and overtraining.\n\n\nMax hangs in half crimp, open hand, on a sloper, pinching, etc.: 7-10s on a 20mm edge. One arm or two depends on how much weight you need to add; at some point, it makes more sense to take a little weight off and assess one-armed, with the added benefit of checking for left/right asymmetries.\n\n\nMobility: hamstringsForward bend though it isolates the hamstrings better on the floor and, with a strap, pull your leg straight and measure the angle it makes with the floor. 60-70° is tight; 90° is ok; 105° is awesome.; anklesHow far past your toes can your knee reach; aim for at least 5”, higher is better.; hip turnoutHow close to the wall can your body get with knees frogged, thighs parallel to the floor; hip bones should be at most 10” from the wall, closer is better.; high stepHow high can you step with your body about a foot from the wall? Make sure not to lean sideways!; shoulders\n\n\nPull-ups: Unweighted pull-ups are to endurance as weighted single reps are to strength, as 5-rep weighted pull-ups are to strength endurance. Aim for 5-rep strength around 80% of single rep strength, and single rep strength 75-100% of finger strength.\n\n\nCore: (on the floor) forearm plank; (hanging from a bar) knees tucked, or legs straight (L-sit); then front lever progression: knees tucked, one leg extended, legs wide, both legs extended. Pick the hardest 1-2 you can do 5-30s.\n\n\nPower: explosive pull-ups / muscle-ups for pulling power; assess your contact strength with system board climbs that challenge it, or on a campus board. Maybe your gym has hangboard with a time-resolved force meter.\n\n\nLegs: Jump tapsChalk your hands and see how high you can tap a brick wall; count the bricks to measure distance.; toe the four cornersBalance on one foot and with the other nudge a small object as far forward as you can without putting weight into that foot. Then to the back, then the left and the right. It requires good balance which tests the stable leg (and core) in many positions.; pistol squats (surely just a few are enough).\n\n\nPush-ups / Dips: Useful for mantelling but stop at 20-40. Don’t neglect the other antagonists.\n\n\nAssess your strengths and train your weakest aspects. Finger strength? Pull-ups? Body tension? Steep compression climbs? Scary slab climbs? All/none of them? Which are most crucial for your next goal? What are the lowestUsually the big muscles respond the fastest to training. Any weakness that you’ve never trained will likely see fast improvement once you start. hanging fruit or which take the longestFinger strength develops slowly and is arguably the most important. Mobility may come easily for you but it’s hard-won tiny gains for me; you can never be too mobile (but you can be too flexible). and should be ongoing?"
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#ml-conclusions",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#ml-conclusions",
    "title": "Assess Yourself for Better Bouldering",
    "section": "ML Conclusions",
    "text": "ML Conclusions\n\nWe have a single snapshot of each climber, not their individual progression, certainly not the interventions (aka, the training) they undertook to progress in their climbing. That means our models are merely descriptive and cannot make “causal” conclusions. Still, we can expect that, if not on an individual level, then at least in aggregate, the (mean) V6 climber progresses into a V9 climber, and might progress into a V12 climber, so that the model is at least a progressive description.\n\n\nA model is only as good as the dataset it learns from: while ours clearly found stronger climbers climbed harder grades, it found that all forms of endurance, leg, core, and mobility training lowered expected grade. These are the biases and habits of these climbers, at best a snapshot of training advice from a decade ago; certainly not how the elite climbers train today.\n\n\nOther datasets exist, most notably the scrapped submissions from 8a.nu downloadable from Kaggle. I have plans to test a graph neural network to look for climbing cliques, correlated climb progressions, etc.\n\n\n\n\nAs a quick comparison, with only mild data massaging, the grade vs adjustedSomewhat skewed as the data is actually years on 8a.nu that I adjusted using the trend of grade vs years on 8a.nu; this should still underestimate the number of years climbing since the only data I have on each climber begins when they join the site. Some join from the start, others wait until their first V10. years climbing. The trends are reassuringly similar to ours. Each climber now has one point per year that they posted to 8a.nu."
  },
  {
    "objectID": "posts/assess-yourself-for-better-bouldering/index.html#resources-and-references",
    "href": "posts/assess-yourself-for-better-bouldering/index.html#resources-and-references",
    "title": "Assess Yourself for Better Bouldering",
    "section": "Resources and References",
    "text": "Resources and References\n\nLattice Training released an interesting finger strength vs V grade that compared well with our climbers. They offer a free finger assessment if you want to compare your finger strength against their dataset (and add to it). A video with the Bobats showed them going through a thorough Lattice Training assessment that included various measures of strength, power, endurance, and mobility.\n\n\nThe Substr8 Climbing Performance assessment includes the plank, max hangs, number of pull-ups, and an interesting test of strength endurance: do 3 inclined ring pull-ups at 45° every 10s for as long as possible (up to 6min), “resting” in between but always inclined.\n\n\nSteve Bechtel at Climb Strong has a good intro to Quantifying Bouldering Sessions, including monitoring sum/average of the grades sent per session, mentioned in the text, and “session density”, sum(V)/duration of session, as a quick measure of load.\n\n\nSorely missing in my analysis and poorly treated in most training books: improving your mobility for climbing (and life) may actually yield the greatest gains. Climbing specific routines from Lattice Training (overview, shoulders and lower body) and Hooper’s Beta (shoulders, hips and lower body); or if you prefer a program, I’m in level 2/3 of Calisthenic Movement’s mobility program and very pleased with my progress and how well it goes with climbing.\n\n\nBooks and Papers\n\n\nBeastmaking: a fingers-first approach to becoming a better climber, by Ned Feehally. Certainly biased toward climbing foremost, supplemented with hangboard and system board training, he explains from grip outward how to train strength, power, core and footwork. Although he emphasizes the importance of mobility, he doesn’t cover this well.\n\n\nThe Science and Practice of Strength Training is a wonderful, (nearly fully) self-contained book on strength training intended for coaches and trainers to learn how personalize training for individuals. Discusses strength vs power in detail; strength scaling with weight; strength through muscle systems about a joint (or multiple joints). Pleases my inner physicist and yet the math is fairly straightforward and should be alright for most.\n\n\nTraining for the New Alpinism by Steve House and Scott Johnston. For anyone interested in self-coaching for, possibly, multiple disciplines (aka, trail running and bouldering), this book explains the physiology of endurance vs strength training and the phases of each training cycle. The athlete tales are great reads and the (mountain) photos are spectacular.\n\n\nAll-age relationship between arm span and height in different ethnic groups, Philip H. Quanjer, et. al. European Respiratory Journal 2014 44: 905-912"
  },
  {
    "objectID": "posts/reading-recommendations.html",
    "href": "posts/reading-recommendations.html",
    "title": "References for Data, ML, Math etc.",
    "section": "",
    "text": "Excellent posts:\n\nRules of ML from Google\nA Recipe for Training Neural Networks by Andrej Karpathy\n\nI learn best from reading, not coursera-like videos (though who didn’t enjoy Andrew Ng’s course?). Papers are important to keep up with the latest, but to fill in the background usually left unexplained and to expose the bits I never knew I didn’t know: I turn to textbooks.\nStarting out, I read a bunch of textbooks and as far as I know these are still good starting points:\n\nNeural Networks and Deep Learning by Michael Nielsen: back prop, loss functions, problems when neural networks go deep\nMachine Learning Yearning by Andrew Ng: how to train a model and ensure it works in the real world\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron: it’s all in the title, great book\n\nOn more specific topics, I can recommend:\n\nInterpretable Machine Learning by Christoph Molnar: discusses the inherently explainable models and how to interpret black-box models.\nReinforcement Learning by Sutton and Barto: a slow building up of concepts to some of the most powerful RL algorithms and the language to understand the latest research.\nDesigning Data-Intensive Applications by Martin Kleppman: brilliant book not specifically about data science. Read this to understand the difficulties navigating big data.\nMining of Massive Datasets by Rajaraman and Ullman: a surprising deep and thorough book on scaling algorithms to big data, covering map-reduce, stream processing, PageRank and recommenders at scale.\n\nBeyond purely technical topics, I also suggest:\n\nUX for beginners as a quick intro to an important role in product development that (can) interact with data scientists\nWeapons of Math Destruction on the damage of unacknowledged bias in models\nGödel’s Proof because it’s linked to the “No free lunch” theorem and generally mind-blowing"
  },
  {
    "objectID": "posts/2024-plans.html",
    "href": "posts/2024-plans.html",
    "title": "2024 Update",
    "section": "",
    "text": "Currently working on:\n\nCompleting / writing up / deploying the boulder recommender that I started while taking Stanford CS224U, online lectures (from my cohort!) + github\nStanford CS236; next: Stanford CS230; immunology course? The lectures are typically lightweight enough to watch while exercising\nNaNoWriMo if only to get my time travel story down as a short story or novella\nfinger strength\nanother half marathon in a race or unofficially looping Stanley park, aiming to break 1:45 (last time managed 1:49 but that was with a lot of downhill! Just beating that time on flat would be great. In fact, that race still has my fastest 5km effort according to Strava: I should beat that too!)\n\nWorkwise:\n\nStill at Tableau. In early 2023, when all the layoffs happened, we lost our ML manager and the ML team was dissolved and moved to Salesforce. I was on the Einstein Discovery team for a few months when they realized they had no ML expertise left in Tableau and I was brought back. I’ve since helped bring back 2 more of the original team and hired another 3 ML people to Tableau. Tableau even has a VP AI now!\nWe’re considered “tech flex” and required to come in 10 days a quarter (but that may go up?).\nI had so much holiday I’m taking a week off every month Aug-Dec this year!\n\nTechwise:\n\nSwitched the site to Quarto: so easy, so pretty, and &lt;coming here soon&gt; interactive vizzes!\nAfter a year or 2-3(?) of Ubuntu desktop, I went back to Mac OS with a macbook air\n~3 years ago, I tried an iPhone and did I ever regret it! While I like iOS on my iPad,on the phone it was dreadful (after android) and the photos weren’t even that good. I promptly replaced it with a Sony that takes much better photos\nNow using VS Code (since I work in more than just Python).\nTried the Arc browser and want to like it… Was a Firefox devotee but recently switching to Brave because of the built in ad blocking that works in iOS (for watching lectures in youtube on my ipad without interrupting ads while sweating on the elliptical / peloton)."
  },
  {
    "objectID": "posts/learning-to-learn-without-forgetting-summary.html",
    "href": "posts/learning-to-learn-without-forgetting-summary.html",
    "title": "Learning to learn without forgetting: a summary",
    "section": "",
    "text": "Matthew Riemer et. al. Learning to Learn Without Forgetting by Maximizing Transfer and Minimizing Interference. ICLR, 2019.\n\n\n\nRecall catastrophic forgetting, a neural network sequentially trained on multiple tasks forgets earlier tasks with each new task, apparently not a problem in Bayesian networks\n\n\nWhy? overwriting weights with updates, …\n\n\nHow to avoid? limit weight sharing, balance network stability vs plasticity (“recall of old tasks” versus “rapid learning of new ones”), …\n\n\nThe loss function: \\[\\sum_{i, j} L(x_i , y_i ) + L(x_j , y_j ) − \\alpha {\\partial L(x_i , y_i ) \\over \\partial \\theta} \\cdot {\\partial L(x_j , y_j ) \\over \\partial \\theta}\\]\n\n\nThe regularizing term is a measure of transfer or interference between updates. The gradient wrt to learning parameters guides the backprop update to those parameters: alignment of gradients means the updates agree and will guide learning for both examples; anti-alignment means updates cancel and neither example will learn; any intervening overlap is deemed transfer (interference) for positive (negative) values.\n\n\nMaximizing weight sharing maximizes transfer; minimizing weight sharing minimizes the change for interference.\n\n\nWork leading up to this paper, both offline algorithms over dataset D:\n\n\nMAML -&gt; FOMAML (Finn & Levine, 2017)\n\n\nReptile (Nichol & Schulman, 2018)\n\n\n\n\nContributions: new algorithm MER, meta experience replay, an online algorithm (algorithms 1 with variants 6 & 7):\n\n\nadded an inner loop within Reptile batches for an inner meta-learning update\n\n\nkeeps a memory/reservoir of examples M to approximate the full dataset D with new examples added probabilistically to replace old ones (see algorithm 3 in the paper)\n\n\nprioritizes learning of the current examples, esp. because it may not be saved\n\n\n\n\nFirst, the reptile algorithm:\n\n\nfor each epoch of training, \\(t\\), record the current params, \\(\\theta^A_0 = \\theta_{t-1}\\) and sample \\(s\\) batches of size \\(k\\)\n\n\nperform a normal epoch of training over the \\(s\\) batches with learning rate \\(\\alpha\\) toward final params \\(\\theta^A_s\\)\n\n\nupdate the network weights for this epoch only a fraction of the learned param changes: \\[\\theta_t = \\theta^A_0 + \\gamma (\\theta^A_s - \\theta^A_0)\\]\n\n\nthis meta-learning update enacts the effective loss \\[2\\sum_{i=1}^s L(B_i) - \\sum_{j=1}^{i-1} {\\partial L(B_i) \\over \\partial \\theta} \\cdot {\\partial L(B_j) \\over \\partial \\theta}\\]\n\n\n\n\nMER adds a second meta-learning update within each of the \\(s\\) batches, now sampled from reservoir M, each of which will have the current example in it; finally, the reservoir is updated (maybe)\n\n\nfor each epoch of training, \\(t\\), record the current params, \\(\\theta^A_0 = \\theta_{t-1}\\) and sample \\(s\\) batches of size \\(k\\), include example \\(x_t, y_t\\) in each\n\n\nfor each batch \\(i\\), record the current params, \\(\\theta^A_{i, 0} = \\theta^A_{i-1}\\)\n\n\nfor each example \\(j\\) in the batch, perform a backprop update with learning rate \\(\\alpha\\) to params \\(\\theta^A_{i, j}\\)\n\n\nafter the entire batch has been singly learned, meta-learn the parameter update \\[\\theta^A_i = \\theta^A_{i, 0} + \\beta (\\theta^A_{i, k} - \\theta^A_{i, 0})\\]\n\n\nthe effective loss is \\[2\\sum_{i=1}^s \\sum_{j=1}^k L(x_{ij}, y_{ij}) - \\sum_{q=1}^{i-1}\\sum_{r=1}^{j-1} {\\partial L(x_{ij}, y_{ij}) \\over \\partial \\theta} \\cdot {\\partial L(x_{qr}, y_{qr}) \\over \\partial \\theta}\\]\n\n\nnote that they update the batch examples singly to maximize the regularizing effect\n\n\nalgorithms 6 & 7 are alternate ways of prioritizing the current example\n\n\n\n\nEvaluation metrics:\n\n\nlearning accuracy (LA): average accuracy for each task immediately after it has been learned\n\n\nretained accuracy (RA): final retained accuracy across all tasks learned sequentially\n\n\nbackward transfer and interference (BTI): the average change in accuracy from when a task is learned to the end of training (positive good; large and negative is catastrophic forgetting)\n\n\n\n\nProblems:\n\n\nin supervised learning: MNIST permutations, each task is transformed by a fixed permutation of the MNIST pixels; MNIST rotations, each task contains digits rotated by a fixed angle between 0 and 180 degrees; Omniglot, each task is one of 50 alphabets with overall 1623 characters\n\n\nin reinforcement learning: Catcher, a board moved left/right to catch a more and more rapidly falling object; Flappy Bird must fly between ever tightening pipes\n\n\n\n\nCompared against:\n\n\nonline, same network trained straightforwardly one example at a time on the incoming non-stationary training data by simply applying SGD\n\n\nindependent, one model per task with size of network reduced proportionally to keep total number of parameters fixed\n\n\ntask input, trained as in online with a dedicated input layer per task\n\n\nEWC, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), ~online regularized to avoid catastrophic forgetting\n\n\nGEM: Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) uses episodic storage to modify gradients of latest example to not interfere with past ones; stored examples are not used in ongoing training Findings:\n\n\nMER seems to do learn and retain the most over all tasks, faster, and with less memory\n\n\nmy reservations:\n\n\nmnist again?\n\n\nomniglot is not usually studied with any of the algorithms compared against: in Lake (2015) they achieve &lt;5% error rate, still &lt;15% in a stripped down version of their model and 2 out of 3 of their baselines\n\n\nhow much slower will the training be with single example batches and two meta-learning updates?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lara Thompson:: **M**L/**A**I/**D**ata **Scientist**",
    "section": "",
    "text": "Studied physics, math, ~neuroscience; now do ML. \nBoulder and run, read a lot, write some, always learning.\n\nML because it felt natural after physics, closer to what matters in the day to day, exciting in how powerful it can be, challenging in how badly it can fail; mixing math, programming, and problem solving to create products and services that help people rather than manipulate or merely distract, and build technology that means less waste and instead of yet more stuff.\n\n\n\n\n\n\nI post stuff sporadically: notes for papers we’re reading at the Learn Data Science Meetup; data analysis of something fun; write up of concepts I worked through.\n\n\n\n\n\n2024 Update\n\n\nbest laid plans…\n\n\nRandom list of updates and imminent plans.\n\n\n\n\n\nOct 19, 2024\n\n\n2 min\n\n\n\n\n\n\n\nEmbeddings\n\n\nThey aren’t just for text\n\n\nSome introductory notes on embeddings as pre-reading for our data science paper reading group.\n\n\n\n\n\nJul 12, 2023\n\n\n5 min\n\n\n\n\n\n\n\nHow Bad is Top-K Recommendation under Competing Content Creators?\n\n\nIntroduction for Newbies and Summary for the Rushed\n\n\nBackround notes on recommenders from a game theory / bandit learning perspective to help people understand this week’s paper.\n\n\n\n\n\nJul 4, 2023\n\n\n8 min\n\n\n\n\n\n\n\nAssess Yourself for Better Bouldering\n\n\nData-Backed Training Advice\n\n\nAnalysis of a climbing survey dataset to test the wisdom of the crowd in how best to improve at climbing and assess training progress.\n\n\n\n\n\nDec 15, 2021\n\n\n33 min\n\n\n\n\n\n\n\nData is everywhere!\n\n\nUh, where again?\n\n\nLast night I was on a data science career panel (of awesome ladies!) as part the Vancouver Datajam 2020 and I promised (as I’ve been meaning to do for a while…) to post a list of data resources.\n\n\n\n\n\nSep 13, 2020\n\n\n7 min\n\n\n\n\n\n\n\nGPU for ML only on Ubuntu\n\n\n\n\n\nTo go against the grain, you have to fight.\n\n\n\n\n\nSep 11, 2020\n\n\n6 min\n\n\n\n\n\n\n\nSite overhaul to get rid of Jekyll\n\n\n…and get things working again!\n\n\nIf you know html+css, why use a framework? Turns out, a few reasons.\n\n\n\n\n\nSep 7, 2020\n\n\n4 min\n\n\n\n\n\n\n\nReferences for Data, ML, Math etc.\n\n\n\n\n\nSome dated and more or less relevant references to get started in ML.\n\n\n\n\n\nSep 19, 2019\n\n\n2 min\n\n\n\n\n\n\n\nLearning to learn without forgetting: a summary\n\n\n\n\n\nBullet-point form summary of this excellent paper. Contributions include a memory store of past examples; balances learning of new updates with recall of these stored examples; continual (online) learning in supervised and reinforcement learning settings.\n\n\n\n\n\nAug 28, 2019\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  }
]